{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a8ff90c5",
   "metadata": {},
   "source": [
    "# A Deep Learning Heteroscedastic Uncertainty Approach to Fault Detection of Wind Turbines using SCADA data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ac7634",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from math import sqrt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, Dense, Flatten, BatchNormalization, Concatenate, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow_probability import distributions as tfd\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "394b7aac",
   "metadata": {},
   "source": [
    "##### Flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b87a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to true if you run on Google Colab\n",
    "COLAB = False\n",
    "\n",
    "# Set TRAIN to True if want to retrain the models\n",
    "TRAIN = True\n",
    "\n",
    "# Set EVALUATE_FEATURE_IMPORTANCE to True if want to evaluate the feature importance for the BNN model\n",
    "EVALUATE_FEATURE_IMPORTANCE = False\n",
    "\n",
    "# Set confidence interval to be considered as 'normal behaviour'\n",
    "CONFIDENCE_INTERVAL = 0.99\n",
    "\n",
    "HUNDRED = 100\n",
    "TWENTY = 20\n",
    "\n",
    "GPU = True\n",
    "if GPU:\n",
    "    tf.config.set_visible_devices([], 'GPU')\n",
    "    print(\"Using GPU\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2e1dd172",
   "metadata": {},
   "source": [
    "## Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494e81ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to ensure that each model has repeatable results,we fix the seed both for the\n",
    "# data splitting part and for the initilialization of the networks' weights. Theoretially\n",
    "# speaking, we should average over different seeds to ensure the robustness of our results.\n",
    "# However, in practice, due to the size of the data set this is unfeasibile and we only do\n",
    "# this for the best performing model to show that the variability of results based on seed\n",
    "# is almost none.\n",
    "\n",
    "keras.utils.set_random_seed(812)\n",
    "MODELS_SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a38d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \".\"\n",
    "\n",
    "if COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    DATA_PATH = \"/content/drive/My Drive/Colab Notebooks/FYP/UK Wind Turbines\"\n",
    "\n",
    "df_train = pd.read_pickle(open(DATA_PATH + '/cleaned_data_train.pkl','rb'))\n",
    "df_test = pd.read_pickle(open(DATA_PATH + '/cleaned_data_test.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc7ad0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0925e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datetime column\n",
    "DATETIME_COL = 'Date.time'\n",
    "\n",
    "units = {\n",
    "'Wind.speed.me': 'm/s',\n",
    "'Wind.speed.sd': 'm/s', \n",
    "'Wind.speed.min': 'm/s',\n",
    "'Wind.speed.max': 'm/s',\n",
    "'Front.bearing.temp.me': '°C',\n",
    "'Front.bearing.temp.sd': '°C',\n",
    "'Front.bearing.temp.min': '°C',\n",
    "'Front.bearing.temp.max': '°C',\n",
    "'Rear.bearing.temp.me': '°C',\n",
    "'Rear.bearing.temp.sd': '°C',\n",
    "'Rear.bearing.temp.min': '°C',\n",
    "'Rear.bearing.temp.max': '°C',\n",
    "'Rotor.bearing.temp.me': '°C',\n",
    "'Stator1.temp.me': '°C',\n",
    "'Nacelle.ambient.temp.me': '°C',\n",
    "'Nacelle.temp.me': '°C',\n",
    "'Transformer.temp.me': '°C',\n",
    "'Gear.oil.temp.me': '°C',\n",
    "'Gear.oil.inlet.temp.me': '°C',\n",
    "'Top.box.temp.me': '°C',\n",
    "'Hub.temp.me': '°C',\n",
    "'Conv.Amb.temp.me': '°C',\n",
    "'Rotor.bearing.temp.me': '°C',\n",
    "'Transformer.cell.temp.me': '°C',\n",
    "'Motor.axis1.temp.me': '°C',\n",
    "'Motor.axis2.temp.me': '°C',\n",
    "'CPU.temp.me': '°C',\n",
    "'Blade.ang.pitch.pos.A.me': '°',\n",
    "'Blade.ang.pitch.pos.B.me': '°',\n",
    "'Blade.ang.pitch.pos.C.me': '°',\n",
    "'Gear.oil.inlet.press.me': 'bar',\n",
    "'Gear.oil.pump.press.me': 'bar',\n",
    "'Drive.train.acceleration.me': 'mm/s^2',\n",
    "'Tower.Acceleration.x': 'mm/s^2',\n",
    "'Tower.Acceleration.y': 'mm/s^2'\n",
    "}\n",
    "\n",
    "# Features considered\n",
    "features = [\n",
    "'Wind.speed.me',\n",
    "'Wind.speed.sd',\n",
    "'Wind.speed.min',\n",
    "'Wind.speed.max',\n",
    "'Front.bearing.temp.me',\n",
    "'Front.bearing.temp.sd',\n",
    "'Front.bearing.temp.min',\n",
    "'Front.bearing.temp.max',\n",
    "'Rear.bearing.temp.me',\n",
    "'Rear.bearing.temp.sd',\n",
    "'Rear.bearing.temp.min',\n",
    "'Rear.bearing.temp.max',\n",
    "'Rotor.bearing.temp.me',\n",
    "'Stator1.temp.me',\n",
    "'Nacelle.ambient.temp.me',\n",
    "'Nacelle.temp.me',\n",
    "'Transformer.temp.me',\n",
    "'Gear.oil.temp.me',\n",
    "'Gear.oil.inlet.temp.me',\n",
    "'Top.box.temp.me',\n",
    "'Hub.temp.me',\n",
    "'Conv.Amb.temp.me',\n",
    "'Transformer.cell.temp.me',\n",
    "'Motor.axis1.temp.me',\n",
    "'Motor.axis2.temp.me',\n",
    "'CPU.temp.me',\n",
    "'Blade.ang.pitch.pos.A.me',\n",
    "'Blade.ang.pitch.pos.B.me',\n",
    "'Blade.ang.pitch.pos.C.me',\n",
    "'Gear.oil.inlet.press.me',\n",
    "'Gear.oil.pump.press.me',\n",
    "'Drive.train.acceleration.me',\n",
    "'Tower.Acceleration.x',\n",
    "'Tower.Acceleration.y'\n",
    "]\n",
    "\n",
    "output_feature = 'Power.me'\n",
    "TURBINE_ID = 5\n",
    "\n",
    "print(\"Full Training Set Size: \", df_train.shape[0])\n",
    "print(\"Full Testing Set Size: \", df_test.shape[0])\n",
    "\n",
    "TURBINE_ID = 5\n",
    "df_train_single = df_train[df_train['turbine'] == TURBINE_ID]#.reset_index(drop=True, inplace=True)\n",
    "df_test_single = df_test[df_test['turbine'] == TURBINE_ID]#.reset_index(drop=True, inplace=True)\n",
    "df_train_single.reset_index(drop=True, inplace=True)\n",
    "print(\"Turbine#{TURBINE_ID} Training Set Size: \", df_train.shape[0])\n",
    "print(\"Turbine#{TURBINE_ID} Testing Set Size: \", df_test.shape[0])\n",
    "df = pd.concat([df_train_single, df_test_single])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57b5fb3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "descriptive_stats = df[features + [output_feature]].describe()\n",
    "pd.set_option('display.max_columns', None)\n",
    "display(descriptive_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348b2ba1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(27, 18))\n",
    "correlation_matrix = df_train_single[features + [output_feature]].corr()\n",
    "heatmap = sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "\n",
    "cbar = heatmap.collections[0].colorbar\n",
    "cbar.ax.tick_params(labelsize=24)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25765544",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_cols = 5\n",
    "num_rows = math.ceil(len(features) / num_cols)\n",
    "\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, 20))\n",
    "\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, feature in enumerate(features):\n",
    "    sns.histplot(df[feature], kde=True, ax=axes[i], color=\"0.3\")\n",
    "    axes[i].set_title(feature)\n",
    "    axes[i].set_xlabel(units[feature])\n",
    "    axes[i].set_ylabel('Count')\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f359ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot Power over Wind Speed\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.scatter(df['Wind.speed.me'], df[output_feature], alpha = .7, color=\"0.3\", linewidth = 0, s = 2)\n",
    "plt.title('Power Output/Wind Speed')\n",
    "plt.xlabel('Wind Speed (m/s)')\n",
    "plt.ylabel('Power Output (kW)')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c762d02",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot Power over Front Bearing Temperature\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.scatter(df['Front.bearing.temp.me'], df[output_feature], alpha = .7, color=\"0.3\", linewidth = 0, s = 2)\n",
    "plt.title('Power Output/Front Bearing Temperature')\n",
    "plt.xlabel('Front Bearing Temperature (°C)')\n",
    "plt.ylabel('Power Output (kW)')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324c3d8d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_power_over_all_features(df, units, features, output_feature, sample_size=5000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8f0c8a24",
   "metadata": {},
   "source": [
    "These plots show, first of all, the non-linearity relationship between power output and operatioanl variables, and secondly, how the variance can significantly vary within features domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b39cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_full, X_test_full,\\\n",
    "    y_train_full, y_test_full,\\\n",
    "    scaler_full = create_design_matrix(df_train, df_test, features, output_feature)\n",
    "\n",
    "X_train, X_test, \\\n",
    "    y_train, y_test, \\\n",
    "    scaler = create_design_matrix(df_train_single, df_test_single, features, output_feature)\n",
    "\n",
    "assert X_train.shape[0] < X_train_full.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4481e233",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "47613d3f",
   "metadata": {},
   "source": [
    "### Deterministic (Homoscedastic) MLP"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9ca34ec8",
   "metadata": {},
   "source": [
    "We develop a deterministic multilayer perceptron (MLP) that uses as a loss function the mean squared error (MSE). The use of MSE as the loss function requires the assumption that the noise is identically and independently distributed accross the domain. The 'identically distributed' assumtpion assumes that the variance of the noise is not a function of the input parameters. Because of this, the deterministic MLP proposed below only accounts for homoscedastic sources of aleatoric and epistemic. However, the amount of error expressed by the model cannot be explicitly divided into epistemic and aleatoric uncertainty as the model does not explicitly output a distribution but a prediction. Confidence intervals can be derived as a function by assuming a gaussian distribution centred in the output of the network and with variance approximated by the MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222e03a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_model_mlp_non_probabilistic(X_train, seed):\n",
    "    keras.utils.set_random_seed(seed)\n",
    "    \n",
    "    inputs = Input(shape=(X_train.shape[1],))\n",
    "    hidden = Dense(80, activation=\"relu\")(inputs)\n",
    "    hidden = Dense(50, activation=\"relu\")(hidden)\n",
    "    hidden = Dense(20, activation=\"relu\")(hidden)\n",
    "    output = Dense(1, activation=\"linear\")(hidden) \n",
    "\n",
    "    model_mlp_non_probabilistic = Model(inputs=inputs, outputs=output)\n",
    "    model_mlp_non_probabilistic.compile(Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "    \n",
    "    return model_mlp_non_probabilistic\n",
    "\n",
    "model_mlp_non_probabilistic = create_model_mlp_non_probabilistic(X_train, MODELS_SEED)\n",
    "model_path = \"mlp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e93f21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN == True:\n",
    "\n",
    "    cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=overwrite(f\"saved_models/{model_path}.weights.h5\"),\n",
    "                                                     save_weights_only=True,\n",
    "                                                     verbose=0)\n",
    "\n",
    "    history = train_model(model_mlp_non_probabilistic,\n",
    "                          X_train, y_train,\n",
    "                          patience=10, epochs=HUNDRED, batch_size=32,\n",
    "                          cp_callback=cp_callback,\n",
    "                          seed=MODELS_SEED)\n",
    "\n",
    "    plot_loss_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979cfa17",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mlp_non_probabilistic.load_weights(f\"saved_models/{model_path}.weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f522c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mlp_non_probabilistic.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f336fb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = np.array(model_mlp_non_probabilistic.predict(X_train)).ravel()\n",
    "y_test_pred = np.array(model_mlp_non_probabilistic.predict(X_test)).ravel()\n",
    "\n",
    "# We approximate the standard deviation of the assumed gaussian noise by using the RMSE of the prediction. This also assumes\n",
    "# homoscedasticity nature of the noise.\n",
    "y_train_stddevs = np.full(len(y_train_pred), np.sqrt(mean_squared_error(y_train, y_train_pred)))\n",
    "y_test_stddevs = np.full(len(y_test_pred), np.sqrt(mean_squared_error(y_test, y_test_pred)))\n",
    "\n",
    "evaluate_and_save_metrics(\"Non-probabilistic MLP\",\n",
    "                           y_train, y_test, y_train_pred, y_test_pred,\n",
    "                           y_train_stddevs, y_test_stddevs, CONFIDENCE_INTERVAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a24dcf0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_means_variances(y_test, y_test_pred, y_test_stddevs, save_path=f\"figures/{model_path}/preds.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee91df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confidence_interval_bar(y_test_pred, y_test_stddevs, y_test, bins=20, save_path=f\"figures/{model_path}/cis.png\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6cd7b9c0",
   "metadata": {},
   "source": [
    "As you can see, althought the standard non-probabilistic neural network is an excellent approximator of the non linear relationship between SCADA variables and power output, as shown by the low RMSE, it is not a good model for evaluating uncertainty in prediction. It poorly overestimates uncertainty for power outputs, underestimates noise for mid-high power outputs, and again overestimates noise for the power outputs approaching 2000kW."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "14378bd1",
   "metadata": {},
   "source": [
    "## Bayesian Neural Networks (/Deterministic Output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "95b33009",
   "metadata": {},
   "source": [
    "The purpose of introducing Bayesian Inference in neural networks is to introduce an estimation of the epistemic uncertainty (i.e., the uncertainty of the model - reducible with further training) of the model. In this framework, the model's weights are represented as posterior distributions instead of point-estimates. Therefore, as compared to traditional neural networks, the purpose of Bayesian Neural Network is to infere weight distributions from which an output given an input can be sampled. This represents the uncertainty of the model due to lack of data coverage. Typically, we expect epistemic uncertainty to be quite low for SCADA data sets as they provide large availability of data. \n",
    "\n",
    "BNN models are typically employed in data scarse systems, where identifying the uncertainty due to the lack of sufficient training is paramount. Because our dataset contains over 150,000 data entries, we expect low levels of epistemic uncertainty. This because it is possible to reduce epistemic uncertainty by giving the model sufficient representability capacity and train it over a sufficiently large dataset with respect to the number of model parameters. Nevertheless, we explore the use of BNN with deterministic output and probabilistic weights as a baseline model, and to show that most of the uncertainty in our regression task is given by irreducible noise and confirm that this is input-dependent. The models presented below leverage the approximate Bayesian Inference approach using Dense Flipout hidden layers. However, they have a non-probabilistic output, meaning it is unable to express heteroscedastic sources of aleatoric uncertainty. Again, because it minimizes the MSE, it assumes the data is distributed around an unknown function we are trying to approximate with an additive noise with homoscedastic variance.  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3799037c",
   "metadata": {},
   "source": [
    "In particular, because of the higher computational complexity associated with BNNs as compared to standard neural networks, we have to reduce the size of our dataset to a 1% random sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a27471",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_PERCENTAGE = 0.01\n",
    "np.random.seed(MODELS_SEED)\n",
    "\n",
    "sampled_indices = np.random.choice(len(X_train), int(len(X_train) * SAMPLE_PERCENTAGE), replace=False)\n",
    "X_train_sampled = X_train[sampled_indices]\n",
    "y_train_sampled = y_train[sampled_indices]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ca8f144a",
   "metadata": {},
   "source": [
    "### Dense Flipout Layers (sample data set)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "99c8cf1e",
   "metadata": {},
   "source": [
    "Below it is presented a Bayesian multilayer perceptron (MLP) which approximates bayesian inference using Variational Inference via Flipout [1]. We specify Normal distributions with trainable parameters as weights for all hidden layers. In this case, we have an heteroscedastic epistemic and homoscedastic aleatoric model. The heteroscedasticity nature of the represented model uncertainty is derived through the use of probability distributions as weights, while the noise is still modelled with fixed variance (aleatoric homoscedasticity) because the network has a deterministic output that minimizes the MSE (this is equivalent to Maximum A Posteriori estimation of a symmetric distribution).\n",
    "\n",
    "\n",
    "[1] Wen, Y., Vicol, P., Ba, J., Tran, D., & Grosse, R. (2018). Flipout: Efficient Pseudo-Independent weight perturbations on Mini-Batches. arXiv (Cornell University). https://doi.org/10.48550/arxiv.1803.04386"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fd9bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_mlp_non_probabilistic_bnn_flipout(X_train, seed):\n",
    "    keras.utils.set_random_seed(seed)\n",
    "    \n",
    "    inputs = Input(shape=(X_train.shape[1],))\n",
    "    \n",
    "    hidden1 = tfp.layers.DenseFlipout(\n",
    "            units=30,\n",
    "            kernel_prior_fn = tfp.layers.default_mean_field_normal_fn(),\n",
    "            kernel_posterior_fn = tfp.layers.default_mean_field_normal_fn() ,\n",
    "            activation = \"relu\"\n",
    "        )(inputs)\n",
    "    \n",
    "    hidden2 = tfp.layers.DenseFlipout(\n",
    "            units=30,\n",
    "            kernel_prior_fn = tfp.layers.default_mean_field_normal_fn(),\n",
    "            kernel_posterior_fn = tfp.layers.default_mean_field_normal_fn() ,\n",
    "            activation = \"relu\"\n",
    "        )(hidden1)\n",
    "    \n",
    "    hidden3 = tfp.layers.DenseFlipout(\n",
    "            units=20,\n",
    "            kernel_prior_fn = tfp.layers.default_mean_field_normal_fn(),\n",
    "            kernel_posterior_fn = tfp.layers.default_mean_field_normal_fn() ,\n",
    "            activation = \"relu\"\n",
    "        )(hidden2)\n",
    "    \n",
    "    output = tfp.layers.DenseFlipout(\n",
    "            units=1,\n",
    "            kernel_prior_fn = tfp.layers.default_mean_field_normal_fn(),\n",
    "            kernel_posterior_fn = tfp.layers.default_mean_field_normal_fn() ,\n",
    "            activation = \"relu\"\n",
    "        )(hidden2)\n",
    "\n",
    "    model_mlp_non_probabilistic_bnn = Model(inputs=inputs, outputs=output)\n",
    "    model_mlp_non_probabilistic_bnn.compile(Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "\n",
    "    return model_mlp_non_probabilistic_bnn\n",
    "\n",
    "print(X_train_sampled.shape[1])\n",
    "\n",
    "model_mlp_non_probabilistic_bnn_flipout_sample = create_model_mlp_non_probabilistic_bnn_flipout(X_train_sampled, MODELS_SEED)\n",
    "model_path = \"bnn_non_prob_sample\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6220d8c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if TRAIN == True:\n",
    "    cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=overwrite(f\"saved_models/{model_path}.weights.h5\"),\n",
    "                                                     save_weights_only=True,\n",
    "                                                     verbose=0)\n",
    "\n",
    "    history = train_model(model_mlp_non_probabilistic_bnn_flipout_sample,\n",
    "                          X_train_sampled, y_train_sampled,\n",
    "                          patience=20, epochs=HUNDRED, batch_size=256,\n",
    "                          cp_callback=cp_callback,\n",
    "                          seed=MODELS_SEED)\n",
    "\n",
    "    plot_loss_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc5bdea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model weights\n",
    "model_mlp_non_probabilistic_bnn_flipout_sample.load_weights(f\"saved_models/{model_path}.weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33f394c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_train_pred_mean, y_train_pred_stddevs, \\\n",
    "y_test_pred_mean, y_test_pred_stddevs = \\\n",
    "compute_predictions(model_mlp_non_probabilistic_bnn_flipout_sample, X_train, X_test, num_samples=TWENTY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b7f022",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = y_train_pred_mean.ravel()\n",
    "y_test_pred = y_test_pred_mean.ravel()\n",
    "\n",
    "y_train_stddevs = y_train_pred_stddevs.ravel()\n",
    "y_test_stddevs = y_test_pred_stddevs.ravel()\n",
    "\n",
    "evaluate_and_save_metrics(f\"Epistemic Uncertainty BNN (Flipout) - {SAMPLE_PERCENTAGE*100}% Sample\", y_train, y_test, y_train_pred, y_test_pred, y_train_stddevs, y_test_stddevs, CONFIDENCE_INTERVAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b366d829",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_means_variances(y_test, y_test_pred, y_test_stddevs, save_path=f\"figures/{model_path}/preds.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00f9e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confidence_interval_bar(y_test_pred, y_test_stddevs, y_test, bins=20, save_path=f\"figures/{model_path}/cis.png\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ec8e4ce2",
   "metadata": {},
   "source": [
    "The RMSE of the of the model is high. We explore if this can be reduced with further training by considering the full training data set."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aa53bce5",
   "metadata": {},
   "source": [
    "### Dense Flipout Layers (full data set)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5f8b1902",
   "metadata": {},
   "source": [
    "An equivalent model as shown above, but trained over the full dataset. We show how further training can reduce epistemic uncertainty and decrease the prediction error of the model. However, we show how this is not fully representative of the uncertainty in the system, since most of the uncertainty is given by the variance of the noise, which we cannot model with a deterministic output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3b6109",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mlp_non_probabilistic_bnn_flipout = create_model_mlp_non_probabilistic_bnn_flipout(X_train, MODELS_SEED)\n",
    "model_path = \"bnn_non_prob\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bfbfb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN == True:\n",
    "    cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=f\"saved_models/{model_path}.weights.h5\",\n",
    "                                                     save_weights_only=True,\n",
    "                                                     verbose=0)\n",
    "\n",
    "    history = train_model(model_mlp_non_probabilistic_bnn_flipout,\n",
    "                          X_train, y_train,\n",
    "                          patience=20, epochs=4*HUNDRED, batch_size=256,\n",
    "                          cp_callback=cp_callback,\n",
    "                          seed=MODELS_SEED)\n",
    "\n",
    "    plot_loss_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e26135",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model weights\n",
    "model_mlp_non_probabilistic_bnn_flipout.load_weights(f\"saved_models/{model_path}.weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cd1f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mlp_non_probabilistic_bnn_flipout.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f980b03",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_train_pred_mean, y_train_pred_stddevs, \\\n",
    "y_test_pred_mean, y_test_pred_stddevs = \\\n",
    "compute_predictions(model_mlp_non_probabilistic_bnn_flipout, X_train, X_test, num_samples=TWENTY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d811889",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = y_train_pred_mean.ravel()\n",
    "y_test_pred = y_test_pred_mean.ravel()\n",
    "\n",
    "y_train_stddevs = y_train_pred_stddevs.ravel()\n",
    "y_test_stddevs = y_test_pred_stddevs.ravel()\n",
    "\n",
    "evaluate_and_save_metrics(\"Epistemic Uncertainty BNN (Flipout)\", y_train, y_test, y_train_pred, y_test_pred, y_train_stddevs, y_test_stddevs, CONFIDENCE_INTERVAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62faab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_means_variances(y_test, y_test_pred, y_test_stddevs, save_path=f\"figures/{model_path}/preds.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3341a8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confidence_interval_bar(y_test_pred, y_test_stddevs, y_test, bins=20, save_path=f\"figures/{model_path}/cis.png\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f177ca8f",
   "metadata": {},
   "source": [
    "##### Feature Importance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cc3bbe78",
   "metadata": {},
   "source": [
    "For the interest of the field we evaluate the raw feature importance within the Bayesian network. Let's evaluate feature importance for the BNN with deterministic output. We do this only for the deterministic output because the RATE library does not support probabilistic regressions (see https://github.com/lorinanthony/RATE). To do so we retrain the model wrapped into a BNN_Regressor class from the RATE library."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "82f326f8",
   "metadata": {},
   "source": [
    "For installation, see https://github.com/lorinanthony/RATE/tree/master/Software/rate-bnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060147e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if EVALUATE_FEATURE_IMPORTANCE == True:\n",
    "    from rate import *\n",
    "    \n",
    "    p = X_test.shape[1]\n",
    "\n",
    "    kl_divergence_fn=lambda q, p, _: tfp.distributions.kl_divergence(q, p) / (X_train.shape[0] * 1.0)\n",
    "\n",
    "    layers = []\n",
    "\n",
    "    layers.append(tfp.layers.DenseFlipout(\n",
    "            units=30,\n",
    "            input_shape=(p,),\n",
    "            kernel_prior_fn = tfp.layers.default_mean_field_normal_fn(),\n",
    "            kernel_posterior_fn = tfp.layers.default_mean_field_normal_fn() ,\n",
    "            kernel_divergence_fn = kl_divergence_fn,\n",
    "            activation = \"relu\"\n",
    "        ))\n",
    "\n",
    "    layers.append(tfp.layers.DenseFlipout(\n",
    "            units=30,\n",
    "            kernel_prior_fn = tfp.layers.default_mean_field_normal_fn(),\n",
    "            kernel_posterior_fn = tfp.layers.default_mean_field_normal_fn() ,\n",
    "            kernel_divergence_fn = kl_divergence_fn,\n",
    "            activation = \"relu\"\n",
    "        ))\n",
    "\n",
    "    layers.append(tfp.layers.DenseFlipout(\n",
    "            units=20,\n",
    "            kernel_prior_fn = tfp.layers.default_mean_field_normal_fn(),\n",
    "            kernel_posterior_fn = tfp.layers.default_mean_field_normal_fn() ,\n",
    "            kernel_divergence_fn = kl_divergence_fn,\n",
    "            activation = \"relu\"\n",
    "        ))\n",
    "\n",
    "    layers.append(tfp.layers.DenseFlipout(\n",
    "            units=1,\n",
    "            kernel_prior_fn = tfp.layers.default_mean_field_normal_fn(),\n",
    "            kernel_posterior_fn = tfp.layers.default_mean_field_normal_fn() ,\n",
    "            kernel_divergence_fn = kl_divergence_fn,\n",
    "            activation = \"linear\"\n",
    "        ))\n",
    "\n",
    "    bnn = BNN_Regressor(layers, p=p)\n",
    "    fit_history = bnn.fit(X_train, y_train, epochs=HUNDRED//2, batch_size=256, validation_split=0.1,\n",
    "                        callbacks=[EarlyStopping(monitor=\"val_loss\", patience=5)],\n",
    "                        verbose=0)\n",
    "\n",
    "    sampled_indices = np.random.choice(len(X_train), 10000, replace=False)\n",
    "    X_test_sampled = X_train[sampled_indices]\n",
    "    y_test_sampled = y_train[sampled_indices]\n",
    "\n",
    "    ratings = RATE_BNN(bnn, X_test_sampled, n_workers=1)\n",
    "\n",
    "    sampled_indices = np.random.choice(len(X_train), 10000, replace=False)\n",
    "    X_test_sampled = X_train[sampled_indices]\n",
    "    y_test_sampled = y_train[sampled_indices]\n",
    "\n",
    "    rates = RATE_BNN(bnn, X_test_sampled, n_workers=1)\n",
    "    feature_importance = {feature: rate for feature, rate in zip(features, list(rates[0]))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc57cdf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if EVALUATE_FEATURE_IMPORTANCE == True:\n",
    "    features = list(feature_importance.keys())\n",
    "    importance_values = list(feature_importance.values())\n",
    "\n",
    "    plt.figure(figsize=(10, 12))\n",
    "    plt.barh(features, importance_values, color='skyblue')\n",
    "    plt.xlabel('Importance')\n",
    "    plt.ylabel('Features')\n",
    "    plt.title('Feature Importance')\n",
    "\n",
    "    for i, importance in enumerate(importance_values):\n",
    "        plt.text(importance, i, f'{importance*100:.2f}', va='center')\n",
    "\n",
    "    plt.xlim(0, max(importance_values) + 0.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "047f79e8",
   "metadata": {},
   "source": [
    "While the deterministic Bayesian deep model presents very low RMSE, it is a poor approximator of the overall uncertainty of the model. This because it can only express the intrinsic uncertainty of the model caused by insufficient training or lack of expressivity of the model. However, since the epistemic uncertainty is evaluated as very low, it appears that the prediction residuals of the model are mostly caused by noise, which we cannot fully model with a non-probabilistic output. Moreover, the epistemic uncertainty is estimated as independent of the output true values."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ff7ced11",
   "metadata": {},
   "source": [
    "## Bayesian Neural Networks (Probabilistic Output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0a1c2e31",
   "metadata": {},
   "source": [
    "We introduce a probabilistic output to a bayesian netwrok with the same number of layers as the previous but more hidden units. This way we model the output as a univariate distribution parametrized via a mean and a variance functions in the input space of the network. The loss becomes the negative log-likelihood of the identically and indipendently distributed data points. Where the distribution is a univariate Normal with a mean function and variance function paramettrized by the model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "71d783bd",
   "metadata": {},
   "source": [
    "### Dense Flipout + Gaussian Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fdd9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_bnn(X_train, seed):\n",
    "    keras.utils.set_random_seed(seed) \n",
    "    \n",
    "    kl_divergence_fn=lambda q, p, _: tfp.distributions.kl_divergence(q, p) / (X_train.shape[0] * 1.0)\n",
    "        \n",
    "    inputs = Input(shape=(X_train.shape[1],))\n",
    "    \n",
    "    hidden1 = tfp.layers.DenseFlipout(\n",
    "            units=80,\n",
    "            kernel_prior_fn = tfp.layers.default_mean_field_normal_fn(),\n",
    "            kernel_posterior_fn = tfp.layers.default_mean_field_normal_fn() ,\n",
    "            kernel_divergence_fn = kl_divergence_fn,\n",
    "            activation = \"relu\"\n",
    "        )(inputs)\n",
    "    \n",
    "    hidden2 = tfp.layers.DenseFlipout(\n",
    "            units=50,\n",
    "            kernel_prior_fn = tfp.layers.default_mean_field_normal_fn(),\n",
    "            kernel_posterior_fn = tfp.layers.default_mean_field_normal_fn() ,\n",
    "            kernel_divergence_fn = kl_divergence_fn,\n",
    "            activation = \"relu\"\n",
    "        )(hidden1)\n",
    "    \n",
    "    hidden3 = tfp.layers.DenseFlipout(\n",
    "            units=20,\n",
    "            kernel_prior_fn = tfp.layers.default_mean_field_normal_fn(),\n",
    "            kernel_posterior_fn = tfp.layers.default_mean_field_normal_fn() ,\n",
    "            kernel_divergence_fn = kl_divergence_fn,\n",
    "            activation = \"relu\"\n",
    "        )(hidden2)\n",
    "    \n",
    "    # Output Univariate Normal Probabilistic Layer\n",
    "    dist_params = Dense(2)(hidden3)\n",
    "    dist = tfp.layers.DistributionLambda(normal_softplus)(dist_params)\n",
    "\n",
    "    model_bnn = Model(inputs=inputs, outputs=dist)\n",
    "    model_bnn.compile(Adam(learning_rate=0.001), loss=NLL)\n",
    "\n",
    "    return model_bnn\n",
    "\n",
    "model_bnn = create_model_bnn(X_train, MODELS_SEED)\n",
    "model_path = \"bnn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05587d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN == True:\n",
    "    cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=overwrite(f\"saved_models/{model_path}.weights.h5\"),\n",
    "                                                     save_weights_only=True,\n",
    "                                                     verbose=0)\n",
    "    history = train_model(model_bnn,\n",
    "                          X_train, y_train,\n",
    "                          patience=10, epochs=HUNDRED, batch_size=256,\n",
    "                          cp_callback=cp_callback,\n",
    "                          seed=MODELS_SEED)\n",
    "\n",
    "    plot_loss_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5fbb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bnn.load_weights(f\"saved_models/{model_path}.weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddbc93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bnn.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f726ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = np.array(model_bnn(X_train).mean()).ravel()\n",
    "y_test_pred = np.array(model_bnn(X_test).mean()).ravel()\n",
    "\n",
    "y_train_stddevs = np.array(model_bnn(X_train).stddev()).ravel()\n",
    "y_test_stddevs = np.array(model_bnn(X_test).stddev()).ravel()\n",
    "\n",
    "name = \"Stochastic Output BNN (Flipout)\"\n",
    "\n",
    "save_preds(name, y_test_pred, y_test_stddevs)\n",
    "\n",
    "evaluate_and_save_metrics(name,\n",
    "                           y_train, y_test, y_train_pred, y_test_pred,\n",
    "                           y_train_stddevs, y_test_stddevs, CONFIDENCE_INTERVAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7d8a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_means_variances(y_test, y_test_pred, y_test_stddevs, save_path=f\"figures/{model_path}/preds.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053f95c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confidence_interval_bar(y_test_pred, y_test_stddevs, y_test, bins=20, save_path=f\"figures/{model_path}/cis.png\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "230e991c",
   "metadata": {},
   "source": [
    "## Non-Bayesian MLP (Probabilistic Output) \n",
    "We create a non-Bayesian deep model for the mean and the standard deviation of a Gaussian. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b67b6d04",
   "metadata": {},
   "source": [
    "### MLP with Normal Output - Separate Submodels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9cd5ab40",
   "metadata": {},
   "source": [
    "Two identical and parallel networks process the input data independently to estimate the mean and variance parameters. These two sub-models employ three hidden layers with ReLU activation functions to capture non-linearity in the system with increasing levels of abstraction. The final layers of these sub-models are merged into a single output layer with two units, one for the predicted mean and one for the predicted variance). The loss is the negative log-likelihood of the Normal parametrized by these sub-models. This means, that, although the two sub-models extract independent non-linear features, they are trained simultaneously to minimize a common loss. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffb912a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_mlp_gaussian_separate(X_train, seed):\n",
    "    keras.utils.set_random_seed(seed)\n",
    "    \n",
    "    inputs = Input(shape=(X_train.shape[1],))\n",
    "    mean_h1 = Dense(80, activation=\"relu\")(inputs)\n",
    "    variance_h1 = Dense(80, activation=\"relu\")(inputs)\n",
    "    \n",
    "    mean_h2 = Dense(50, activation=\"relu\")(mean_h1)\n",
    "    variance_h2 = Dense(50, activation=\"relu\")(variance_h1)\n",
    "    \n",
    "    mean_h3 = Dense(20, activation=\"relu\")(mean_h2)\n",
    "    variance_h3 = Dense(20, activation=\"relu\")(variance_h2)\n",
    "    \n",
    "    mean_h4 = Dense(20, activation=\"relu\")(mean_h3)\n",
    "    variance_h4 = Dense(20, activation=\"relu\")(variance_h3)\n",
    "    \n",
    "    mean_out = Dense(1)(mean_h4)\n",
    "    variance_out = Dense(1)(variance_h4)\n",
    "    \n",
    "    params = Concatenate()([mean_out, variance_out])\n",
    "    \n",
    "    dist = tfp.layers.DistributionLambda(normal_softplus)(params) \n",
    "\n",
    "    model_mlp_gaussian = Model(inputs=inputs, outputs=dist)\n",
    "    model_mlp_gaussian.compile(Adam(learning_rate=0.001), loss=NLL)\n",
    "\n",
    "    return model_mlp_gaussian\n",
    "\n",
    "model_mlp_gaussian_separate = create_model_mlp_gaussian_separate(X_train, MODELS_SEED)\n",
    "model_path = \"mlp_sep\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093ea748",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN == True:\n",
    "    cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=f\"saved_models/{model_path}.weights.h5\",\n",
    "                                                     save_weights_only=True,\n",
    "                                                     verbose=0)\n",
    "    history = train_model(model_mlp_gaussian_separate,\n",
    "                          X_train, y_train,\n",
    "                          patience=15, epochs=HUNDRED, batch_size=32,\n",
    "                          cp_callback=cp_callback,\n",
    "                          seed=MODELS_SEED)\n",
    "\n",
    "    plot_loss_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e590118",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mlp_gaussian_separate.load_weights(f\"saved_models/{model_path}.weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a96ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mlp_gaussian_separate.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a55f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = np.array(model_mlp_gaussian_separate(X_train).mean()).ravel()\n",
    "y_test_pred = np.array(model_mlp_gaussian_separate(X_test).mean()).ravel()\n",
    "\n",
    "y_train_stddevs = np.array(model_mlp_gaussian_separate(X_train).stddev()).ravel()\n",
    "y_test_stddevs = np.array(model_mlp_gaussian_separate(X_test).stddev()).ravel()\n",
    "\n",
    "evaluate_and_save_metrics(\"Probabilistic MLP (Separate Sub-models)\",\n",
    "                           y_train, y_test, y_train_pred, y_test_pred,\n",
    "                           y_train_stddevs, y_test_stddevs, CONFIDENCE_INTERVAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7eddab",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_means_variances(y_test, y_test_pred, y_test_stddevs, save_path=f\"figures/{model_path}/preds.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e32e85a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_confidence_interval_bar(y_test_pred, y_test_stddevs, y_test, bins=20, save_path=f\"figures/{model_path}/cis.png\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8cd9bdf8",
   "metadata": {},
   "source": [
    "### MLP with Gaussian Output - Joint Submodels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bf334dd0",
   "metadata": {},
   "source": [
    "A shared deep feature extractor sub-model is followed by two shallow sub-models with a single hidden layer and a single-unit output layer respectively. The first shared sub-model extract non-linear features from the input space, and the separate models estimate mean and variance respectively using non-linear combinations of the features extracted by the shared sub-model. The two single-unit outputs of the mean and variance sub-models are concatenated to represent the parameters of a univariate Normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8e4e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_mlp_gaussian_joint(X_train, seed):\n",
    "    keras.utils.set_random_seed(seed)\n",
    "    \n",
    "    inputs = Input(shape=(X_train.shape[1],))\n",
    "    \n",
    "    hidden1 = Dense(100, activation=\"relu\")(inputs)\n",
    "    hidden2 = Dense(80, activation=\"relu\")(hidden1)\n",
    "    hidden3 = Dense(40, activation=\"relu\")(hidden2)\n",
    "    \n",
    "    mean_h1 = Dense(20, activation=\"relu\")(hidden3)\n",
    "    mean_out = Dense(1)(mean_h1)\n",
    "    \n",
    "    variance_h1 = Dense(20, activation=\"relu\")(hidden3)\n",
    "    variance_out = Dense(1)(variance_h1)\n",
    "    \n",
    "    params = Dense(2)(Concatenate()([mean_out, variance_out]))\n",
    "\n",
    "    \n",
    "    dist = tfp.layers.DistributionLambda(normal_softplus)(params) \n",
    "\n",
    "    model_mlp_gaussian = Model(inputs=inputs, outputs=dist)\n",
    "    model_mlp_gaussian.compile(Adam(learning_rate=0.001), loss=NLL)\n",
    "\n",
    "    return model_mlp_gaussian\n",
    "\n",
    "model_mlp_gaussian_joint = create_model_mlp_gaussian_joint(X_train, MODELS_SEED)\n",
    "model_path = \"mlp_joint\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5fe66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN == True:\n",
    "    cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=overwrite(f\"saved_models/{model_path}.weights.h5\"),\n",
    "                                                     save_weights_only=True,\n",
    "                                                     verbose=0)\n",
    "    history = train_model(model_mlp_gaussian_joint,\n",
    "                          X_train, y_train,\n",
    "                          patience=15, epochs=HUNDRED, batch_size=32,\n",
    "                          cp_callback=cp_callback,\n",
    "                          seed=MODELS_SEED)\n",
    "\n",
    "    plot_loss_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c9db55",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mlp_gaussian_joint.load_weights(f\"saved_models/{model_path}.weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c200ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mlp_gaussian_joint.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a773c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = np.array(model_mlp_gaussian_joint(X_train).mean()).ravel()\n",
    "y_test_pred = np.array(model_mlp_gaussian_joint(X_test).mean()).ravel()\n",
    "\n",
    "y_train_stddevs = np.array(model_mlp_gaussian_joint(X_train).stddev()).ravel()\n",
    "y_test_stddevs = np.array(model_mlp_gaussian_joint(X_test).stddev()).ravel()\n",
    "\n",
    "name = \"Probabilistic MLP (Joint Sub-models)\"\n",
    "\n",
    "save_preds(name, y_test_pred, y_test_stddevs)\n",
    "\n",
    "evaluate_and_save_metrics(name,\n",
    "                           y_train, y_test, y_train_pred, y_test_pred,\n",
    "                           y_train_stddevs, y_test_stddevs, CONFIDENCE_INTERVAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30abcd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_means_variances(y_test, y_test_pred, y_test_stddevs, save_path=f\"figures/{model_path}/preds.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6f78e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_confidence_interval_bar(y_test_pred, y_test_stddevs, y_test, bins=20, save_path=f\"figures/{model_path}/cis.png\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0bee1ba6",
   "metadata": {},
   "source": [
    "By comparing the two architecture it is clear that a separate set of features decreases the model's out-of-sample error. Therefore, the proposed model in the study is the first. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e8594ec3",
   "metadata": {},
   "source": [
    "Below are presented a set of model that have been tested as potential coompetitors to the finalized model above. They have been presented here as they represent more complicated ways of leveraging deep learning as a tool for normal behaviour modelling of wind turbines power curve. However, the increased complexity of the model did not who improvements in performance, therefore the simpler and more robust model presented above was chosen as th ebest candidate."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2aa62d7f",
   "metadata": {},
   "source": [
    "## Additional Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7970c157",
   "metadata": {},
   "source": [
    "### CNN with Gaussian Output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "08ebb5c6",
   "metadata": {},
   "source": [
    "In light of a study conducted by et al. [2], we propose the use of a 1-D CNN to be used as an extractor of spatial features within out feature space. While this model is purely exploratory, we expect this not to excel, because of the almost non-existent spatial dependecy among features. In simpler terms, because CNNs capture spatial features, they are extremely dependent in the order of the features. In our case, the ordering of the features as little predictive significance, so the use of a convolution over the feature space of the input is not really empirically justified.\n",
    "\n",
    "[2] Xiang, L., Wang, P., Yang, X., Hu, A., & Su, H. (2021). Fault de-\n",
    "tection of wind turbine based on SCADA data analysis using CNN\n",
    "and LSTM with an attention mechanism. Measurement, 175, 109094.\n",
    "https://doi.org/10.1016/j.measurement.2021.109094"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a62857d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cnn = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "X_test_cnn = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c489c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_cnn_gaussian(X_train, seed):\n",
    "    keras.utils.set_random_seed(seed)\n",
    "    \n",
    "    inputs = Input(shape=(X_train_cnn.shape[1], 1))\n",
    "    conv1d_layer = Conv1D(filters=32, kernel_size=5, activation='relu')(inputs)\n",
    "    maxpooling_layer = MaxPooling1D(pool_size=2)(conv1d_layer)\n",
    "    \n",
    "    flatten_layer = Flatten()(maxpooling_layer)\n",
    " \n",
    "    hidden1 = Dense(50, activation=\"relu\")(flatten_layer)\n",
    "    hidden2 = Dense(50, activation=\"relu\")(hidden1)\n",
    "    hidden3 = Dense(20, activation=\"relu\")(hidden2)\n",
    "    \n",
    "    params = Dense(2)(hidden3)\n",
    "    dist = tfp.layers.DistributionLambda(normal_softplus)(params) \n",
    "\n",
    "    model_cnn_gaussian = Model(inputs=inputs, outputs=dist)\n",
    "    model_cnn_gaussian.compile(Adam(learning_rate=0.001), loss=NLL)\n",
    "    \n",
    "    return model_cnn_gaussian\n",
    "\n",
    "model_cnn_gaussian = create_model_cnn_gaussian(X_train_cnn, MODELS_SEED)\n",
    "model_path = \"cnn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4cff8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN == True:\n",
    "    cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=overwrite(f\"saved_models/{model_path}.weights.h5\"),\n",
    "                                                     save_weights_only=True,\n",
    "                                                     verbose=0)\n",
    "    history = train_model(model_cnn_gaussian,\n",
    "                          X_train_cnn, y_train,\n",
    "                          patience=15, epochs=HUNDRED, batch_size=32,\n",
    "                          cp_callback=cp_callback,\n",
    "                          seed=MODELS_SEED)\n",
    "\n",
    "    plot_loss_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075a8786",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn_gaussian.load_weights(f\"saved_models/{model_path}.weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00635d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn_gaussian.evaluate(X_test_cnn, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1b25aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = np.array(model_cnn_gaussian(X_train_cnn).mean()).ravel()\n",
    "y_test_pred = np.array(model_cnn_gaussian(X_test_cnn).mean()).ravel()\n",
    "\n",
    "y_train_stddevs = np.array(model_cnn_gaussian(X_train_cnn).stddev()).ravel()\n",
    "y_test_stddevs = np.array(model_cnn_gaussian(X_test_cnn).stddev()).ravel()\n",
    "\n",
    "evaluate_and_save_metrics(\"Probabilistic CNN with Gaussian Output\",\n",
    "                           y_train, y_test, y_train_pred, y_test_pred,\n",
    "                           y_train_stddevs, y_test_stddevs, CONFIDENCE_INTERVAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8785fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_means_variances(y_test, y_test_pred, y_test_stddevs, save_path=f\"figures/{model_path}/preds.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1197a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confidence_interval_bar(y_test_pred, y_test_stddevs, y_test, bins=20, save_path=f\"figures/{model_path}/cis.png\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c921c1d4",
   "metadata": {},
   "source": [
    "Clearly, the model is underperforming as compared to the MLP model with Normal output. This is probably the case because the features extracted by the initial Convolution layer have little significanceas they are dependent on the ordering of the features, which have no spacial dependency. This means the subsequent layers have to unpack the convolution and pooling to learn meaningful features.  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "91223ce3",
   "metadata": {},
   "source": [
    "## Modelling Multiple Turbines: a Multivariate Experiment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "048a315c",
   "metadata": {},
   "source": [
    "We present below an experiment consisting of multi-turbine modelling. The reasoning motivating this research comes from studying and modelling the correlation between different turbines and investigating whether this can improve the predictive performance of the multivariate model. Below are presented deep networks with multivariate outputs for the 6 turbines. The output is a multivariate Gaussian of which we model the mean vector and lower triangular matrix of its Cholesky decomposition. In this case, the negative log likelihood is:\n",
    "\n",
    "$$\n",
    "\\text{NLL}(\\mu, \\Sigma) = \\sum_{i=1}^{n} -\\frac{1}{2} \\left[ d \\cdot \\log(2\\pi) + \\log\\left(|(\\Sigma(\\mathbf{x}_i))|\\right) + (\\mathbf{y}_i - \\boldsymbol{\\mu(\\mathbf{x}_i)})^T (\\Sigma(\\mathbf{x}_i))^{-1} (\\mathbf{y}_i - \\boldsymbol{\\mu(\\mathbf{x}_i)}) \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf11d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_dataframe(df, features, output_feature, date_col):\n",
    "    df['turbine_id'] = df['turbine']\n",
    "    df = df.pivot(index=date_col, columns='turbine_id', values=features + [output_feature])\n",
    "    df.columns = ['_'.join(map(str, col)).strip() for col in df.columns.values]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b469859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the number of turbines to consider\n",
    "turbine_num = 6\n",
    "\n",
    "# Select the n turbines with the most common non-NaN data points\n",
    "turbine_counts = df_train.groupby('turbine').apply(lambda x: x.notna().all(axis=1).sum())\n",
    "turbine_ids = turbine_counts.nlargest(turbine_num).index.tolist()\n",
    "\n",
    "df_top_n = df_train[df_train['turbine'].isin(turbine_ids)]\n",
    "df_multivariate = flatten_dataframe(df_top_n, features, output_feature, DATETIME_COL)\n",
    "df_multivariate.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44fd2ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_multivariate.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2c97b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_feature_columns = [c for c in df_multivariate.columns if c.startswith(output_feature)]\n",
    "feature_columns = list(set(df_multivariate.columns).difference(output_feature_columns))\n",
    "\n",
    "X_train_mult, X_test_mult, y_train_mult, y_test_mult, scaler_x_mult = load_dataset_train_test_split(df_multivariate, feature_columns, output_feature_columns)\n",
    "y_train_mult = np.array(y_train_mult)\n",
    "y_test_mult = np.array(y_test_mult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6bb920",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_mult.shape, y_train_mult.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3185be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of turbines considered in the prediction\n",
    "d = y_train_mult.shape[-1]\n",
    "print(\"Number of turbines considered in the prediction: \", d)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5d020406",
   "metadata": {},
   "source": [
    "### Diagonal Covariance Matrix"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "93f39cec",
   "metadata": {},
   "source": [
    "In this case, we investigate whether we can construct a model which is able to predict all means and variances simultaneously. In this simpler case we do not model covariances between turbines. BY enforcing a diagnonal covariance matrix of the outpyt Normal density we do not model covariances between different turbines, therefore the loss can be decomposed into a sum of individual losses as follows:\n",
    "\n",
    "$$\n",
    "\\mathcal{NLL}_d = \\frac{1}{2} \\sum_{i=1}^{n}\\left[ d \\cdot \\log(2\\pi) + \\log(|\\boldsymbol{\\Sigma_{\\boldsymbol{\\theta}}}(\\mathbf{x}_i)|) + (\\mathbf{y}_i - \\boldsymbol{\\mu_{\\boldsymbol{\\theta}}}(\\mathbf{x}_i))^T \\boldsymbol{\\Sigma^{-1}_{\\boldsymbol{\\theta}}}(\\mathbf{x}_i) (\\mathbf{y}_i - \\boldsymbol{\\mu_{\\boldsymbol{\\theta}}}(\\mathbf{x}_i)) \\right] \\\\\n",
    "= \\frac{1}{2} \\sum_{i=1}^{n}  \\left[ d \\cdot \\log(2\\pi) + \\sum_{j=1}^{d} \\left[ \\log(\\sigma_{\\boldsymbol{\\theta}j}  (\\mathbf{x}_i)) +\\sigma^{-2}_{\\boldsymbol{\\theta}j}(\\mathbf{x}_i)(\\mathbf{y}_{ij} - \\boldsymbol{\\mu}_{\\boldsymbol{\\theta}}(\\mathbf{x}_i))^2 \\right] \\right] \\\\\n",
    "= \\frac{1}{2} \\sum_{i=1}^{n} \\sum_{j=1}^{d} \\left[ \\log(2\\pi) + \\log(\\sigma_{\\boldsymbol{\\theta}j}  (\\mathbf{x}_i)) + \\sigma^{-2}_{\\boldsymbol{\\theta}j}(\\mathbf{x}_i)(\\mathbf{y}_{ij} - \\boldsymbol{\\mu}_{\\boldsymbol{\\theta}j}(\\mathbf{x}_i))^2 \\right]\n",
    "$$\n",
    "where $\\boldsymbol{\\Sigma}_{jj} = \\sigma^{2}_{\\boldsymbol{\\theta}j}$\n",
    "\n",
    "Although the loss is separable, this is not identical to training individual models for each turbine because the input, and possibly network layers, are shared. We investigated two architectures with separate and shared hidden layers and showed that the architecture with separate hidden layers for each turbine produced better predictive results compared to the one with shared layers by allowing the network to learn turbine-specific features."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4a58d086",
   "metadata": {},
   "source": [
    "#### Separate Branches for Separate Turbines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634b9aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_multivariate_gaussian_only_diagonal(d, input_size, seed):\n",
    "    keras.utils.set_random_seed(seed)\n",
    "    \n",
    "    inputs = Input(shape=input_size)\n",
    "    \n",
    "    outs = []\n",
    "\n",
    "    for i in range(d):\n",
    "        \n",
    "        h_means1 = Dense(50, activation='relu')(inputs)\n",
    "        h_cov1 = Dense(50, activation='relu')(inputs)\n",
    "    \n",
    "        h_means2 = Dense(30, activation='relu')(h_means1)\n",
    "        h_cov2 = Dense(30, activation='relu')(h_cov1)\n",
    "    \n",
    "        h_means3 = Dense(20, activation='relu')(h_means2)\n",
    "        h_cov3 = Dense(20, activation='relu')(h_cov2)\n",
    "        \n",
    "        out = Dense(2)(Concatenate()([h_means3, h_cov3]))\n",
    "        outs.append(out)\n",
    "\n",
    "    concatenated_outputs = Concatenate()(outs)\n",
    "    \n",
    "    # Define the distribution layer\n",
    "    distribution_layer = tfp.layers.DistributionLambda(\n",
    "        lambda t: multivariate_diagonal_normal_softplus(t[:, 0::2], t[:, 1::2], d)\n",
    "    )\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=distribution_layer(concatenated_outputs),\n",
    "                  name=\"multivariate_gaussian_with_covariance\")\n",
    "    \n",
    "    model.compile(Adam(learning_rate=0.01),\n",
    "                  loss=NLL)\n",
    "   \n",
    "    return model\n",
    "\n",
    "model_multivariate_gaussian_only_diagonal = create_model_multivariate_gaussian_only_diagonal(d, (X_train_mult.shape[1],), MODELS_SEED)\n",
    "model_path = \"mlp_mult_diag\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695b6e32",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if TRAIN == True:\n",
    "    cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=overwrite(f\"saved_models/{model_path}.weights.h5\"),\n",
    "                                                     save_weights_only=True,\n",
    "                                                     verbose=0)\n",
    "    \n",
    "    history = train_model(model_multivariate_gaussian_only_diagonal,\n",
    "                                X_train_mult, y_train_mult,\n",
    "                                epochs=HUNDRED, patience=10, batch_size=32,\n",
    "                                cp_callback=cp_callback,\n",
    "                                seed=MODELS_SEED)\n",
    "    \n",
    "    plot_loss_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180a8d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_multivariate_gaussian_only_diagonal.load_weights(f\"saved_models/{model_path}.weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3282ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_multivariate_gaussian_only_diagonal.evaluate(X_test_mult, y_test_mult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593d67e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_mult_pred = np.array(model_multivariate_gaussian_only_diagonal(X_train_mult).mean())\n",
    "y_test_mult_pred = np.array(model_multivariate_gaussian_only_diagonal(X_test_mult).mean())\n",
    "\n",
    "y_train_mult_stddevs = np.array(model_multivariate_gaussian_only_diagonal(X_train_mult).stddev())\n",
    "y_test_mult_stddevs = np.array(model_multivariate_gaussian_only_diagonal(X_test_mult).stddev())\n",
    "\n",
    "y_train_mult_covs = np.array(model_multivariate_gaussian_only_diagonal(X_train_mult).covariance())\n",
    "y_test_mult_covs = np.array(model_multivariate_gaussian_only_diagonal(X_test_mult).covariance())\n",
    "\n",
    "evaluate_and_save_metrics(\"Probabilistic Multivariate MLP (Only Diagonal, separate branches)\",\n",
    "                           y_train_mult.ravel(), y_test_mult.ravel(),\n",
    "                           y_train_mult_pred.ravel(), y_test_mult_pred.ravel(),\n",
    "                           y_train_mult_stddevs.ravel(), y_test_mult_stddevs.ravel(),\n",
    "                           CONFIDENCE_INTERVAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec2fbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(y_test_mult_pred.shape[1]):\n",
    "    err = mean_squared_error(y_test_mult[:, i], y_test_mult_pred[:, i])\n",
    "    print(f\"Turbine #{i+1} - RMSE: {sqrt(err)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aba5eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(y_test_mult_pred.shape[1]):\n",
    "    print(f\"Turbine #{i+1}:\") \n",
    "    plot_means_variances(y_test_mult[:, i], y_test_mult_pred[:, i], y_test_mult_stddevs[:, i], save_path=f\"figures/{model_path}/preds{i+1}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78e8dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(y_test_mult_pred.shape[1]):\n",
    "    print(f\"Turbine #{i+1}:\")\n",
    "    plot_confidence_interval_bar(y_test_mult_pred[:, i], y_test_mult_stddevs[:, i], y_test_mult[:, i], bins=20, save_path=f\"figures/{model_path}/cis{i+1}.png\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "56b98590",
   "metadata": {},
   "source": [
    "#### Common Branches for Separate Turbines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77cfae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_multivariate_gaussian_only_diagonal_common(d, input_size, seed):\n",
    "    keras.utils.set_random_seed(seed)\n",
    "    \n",
    "    inputs = Input(shape=input_size)\n",
    "    \n",
    "    outs = []\n",
    "        \n",
    "    h1 = Dense(100, activation='relu')(inputs)\n",
    "    h2 = Dense(50, activation='relu')(h1)\n",
    "    h3 = Dense(30, activation='relu')(h2)\n",
    "    \n",
    "    for i in range(d):\n",
    "        out = Dense(2)(h3)\n",
    "        outs.append(out)\n",
    "\n",
    "\n",
    "    concatenated_outputs = Concatenate()(outs)\n",
    "    \n",
    "    # Define the distribution layer\n",
    "    distribution_layer = tfp.layers.DistributionLambda(\n",
    "        lambda t: multivariate_diagonal_normal_softplus(t[:, 0::2], t[:, 1::2], d)\n",
    "    )\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=distribution_layer(concatenated_outputs),\n",
    "                  name=\"multivariate_gaussian_with_covariance\")\n",
    "    \n",
    "    model.compile(Adam(learning_rate=0.001),\n",
    "                  loss=NLL)\n",
    "   \n",
    "    return model\n",
    "\n",
    "model_multivariate_gaussian_only_diagonal_common = create_model_multivariate_gaussian_only_diagonal_common(d, (X_train_mult.shape[1],), MODELS_SEED)\n",
    "model_path = \"mlp_mult_diag_common\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8475cc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN == True:\n",
    "    cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=overwrite(f\"saved_models/{model_path}.weights.h5\"),\n",
    "                                                     save_weights_only=True,\n",
    "                                                     verbose=0)\n",
    "    \n",
    "    history = train_model(model_multivariate_gaussian_only_diagonal_common,\n",
    "                                X_train_mult, y_train_mult,\n",
    "                                epochs=HUNDRED, patience=10, batch_size=32,\n",
    "                                cp_callback=cp_callback,\n",
    "                                seed=MODELS_SEED)\n",
    "\n",
    "    plot_loss_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8191dd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_multivariate_gaussian_only_diagonal_common.load_weights(f\"saved_models/{model_path}.weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f2b796",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_multivariate_gaussian_only_diagonal_common.evaluate(X_test_mult, y_test_mult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed697b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_mult_pred = np.array(model_multivariate_gaussian_only_diagonal_common(X_train_mult).mean())\n",
    "y_test_mult_pred = np.array(model_multivariate_gaussian_only_diagonal_common(X_test_mult).mean())\n",
    "\n",
    "y_train_mult_stddevs = np.array(model_multivariate_gaussian_only_diagonal_common(X_train_mult).stddev())\n",
    "y_test_mult_stddevs = np.array(model_multivariate_gaussian_only_diagonal_common(X_test_mult).stddev())\n",
    "\n",
    "y_train_mult_covs = np.array(model_multivariate_gaussian_only_diagonal_common(X_train_mult).covariance())\n",
    "y_test_mult_covs = np.array(model_multivariate_gaussian_only_diagonal_common(X_test_mult).covariance())\n",
    "\n",
    "evaluate_and_save_metrics(\"Probabilistic Multivariate MLP (Only Diagonal)\",\n",
    "                           y_train_mult.ravel(), y_test_mult.ravel(),\n",
    "                           y_train_mult_pred.ravel(), y_test_mult_pred.ravel(),\n",
    "                           y_train_mult_stddevs.ravel(), y_test_mult_stddevs.ravel(),\n",
    "                           CONFIDENCE_INTERVAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946ad49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(y_test_mult_pred.shape[1]):\n",
    "    err = mean_squared_error(y_test_mult[:, i], y_test_mult_pred[:, i])\n",
    "    print(f\"Turbine #{i+1} - RMSE: {sqrt(err)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928e237b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(y_test_mult_pred.shape[1]):\n",
    "    print(f\"Turbine #{i+1}:\") \n",
    "    plot_means_variances(y_test_mult[:, i], y_test_mult_pred[:, i], y_test_mult_stddevs[:, i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d6efe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(y_test_mult_pred.shape[1]):\n",
    "    print(f\"Turbine #{i+1}:\")\n",
    "    plot_confidence_interval_bar(y_test_mult_pred[:, i], y_test_mult_stddevs[:, i], y_test_mult[:, i], bins=20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0b0d6647",
   "metadata": {},
   "source": [
    "### Full Covariance Matrix: Lower Triangular Cholesky Decomposed Covariance Matrix"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7c663eff",
   "metadata": {},
   "source": [
    "In the model presented below we model the full covariance matrix of a gaussian distribution. In the world of real numbers, covariance matrices are symmetric and positive-definite. To enforce this we only parametrize a lower triangular matrix as part the Cholesky decomposition of the final full covariance matrix. Therefore, for $d$ turbines, we need $d$ output units for the predicted mean vector and $d(d-1)/2$ for the predicted covariance. So $d(d+1)/2$ in total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787caed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_multivariate_gaussian_with_covariance(d, input_size, seed):\n",
    "    keras.utils.set_random_seed(seed)\n",
    "    \n",
    "    inputs = Input(shape=input_size)\n",
    "\n",
    "    h1_mean = BatchNormalization()(Dense(150, activation='relu')(inputs))\n",
    "    h2_mean = BatchNormalization()(Dense(70, activation='relu')(h1_mean))\n",
    "    h3_mean = BatchNormalization()(Dense(50, activation='relu')(h2_mean))\n",
    "    \n",
    "    h1_cov = BatchNormalization()(Dense(150, activation='relu')(inputs))\n",
    "    h2_cov = BatchNormalization()(Dense(70, activation='relu')(h1_cov))\n",
    "    h3_cov = BatchNormalization()(Dense(50, activation='relu')(h2_cov))\n",
    "    \n",
    "    out_mean = Dense(d, activation='linear')(h3_mean)\n",
    "    out_cov = Dense(tfp.layers.MultivariateNormalTriL.params_size(d) - d)(h3_cov)\n",
    "    \n",
    "    # Concatenate mean and lower triangular part of the covariance matrix\n",
    "    concatenated_outputs = Concatenate()([out_mean, out_cov])\n",
    "    \n",
    "    # Define the distribution layer\n",
    "    distribution_layer = tfp.layers.DistributionLambda(\n",
    "        lambda t: multivariate_covariance_normal_softplus(t[:, :d], t[:, d:], d)\n",
    "    )\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=distribution_layer(concatenated_outputs),\n",
    "                  name=\"multivariate_gaussian_with_covariance\")\n",
    "    \n",
    "    model.compile(Adam(learning_rate=0.001, clipnorm=100),\n",
    "                  loss=NLL)\n",
    "   \n",
    "    return model\n",
    "\n",
    "model_multivariate_gaussian_with_covariance = create_model_multivariate_gaussian_with_covariance(d, (X_train_mult.shape[1],), MODELS_SEED)\n",
    "model_path = \"mlp_mult_full\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ae957b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if TRAIN == True:\n",
    "    cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=f\"saved_models/{model_path}.weights.h5\",\n",
    "                                                     save_weights_only=True,\n",
    "                                                     verbose=0)\n",
    "    \n",
    "    history = train_model(model_multivariate_gaussian_with_covariance,\n",
    "                                X_train_mult, y_train_mult,\n",
    "                                epochs=HUNDRED, patience=10, batch_size=32,\n",
    "                                cp_callback=cp_callback,\n",
    "                                seed=MODELS_SEED)\n",
    "\n",
    "    plot_loss_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd559904",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_multivariate_gaussian_with_covariance.load_weights(f\"saved_models/{model_path}.weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae14184",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_multivariate_gaussian_with_covariance.evaluate(X_test_mult, y_test_mult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a3e7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_mult_pred = np.array(model_multivariate_gaussian_with_covariance(X_train_mult).mean())\n",
    "y_test_mult_pred = np.array(model_multivariate_gaussian_with_covariance(X_test_mult).mean())\n",
    "\n",
    "y_train_mult_stddevs = np.array(model_multivariate_gaussian_with_covariance(X_train_mult).stddev())\n",
    "y_test_mult_stddevs = np.array(model_multivariate_gaussian_with_covariance(X_test_mult).stddev())\n",
    "\n",
    "y_train_mult_covs = np.array(model_multivariate_gaussian_with_covariance(X_train_mult).covariance())\n",
    "y_test_mult_covs = np.array(model_multivariate_gaussian_with_covariance(X_test_mult).covariance())\n",
    "\n",
    "evaluate_and_save_metrics(\"Probabilistic Multivariate MLP (Lower Triangular Covariance Matrix)\",\n",
    "                           y_train_mult.ravel(), y_test_mult.ravel(),\n",
    "                           y_train_mult_pred.ravel(), y_test_mult_pred.ravel(),\n",
    "                           y_train_mult_stddevs.ravel(), y_test_mult_stddevs.ravel(),\n",
    "                           CONFIDENCE_INTERVAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4499c86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(y_test_mult_pred.shape[1]):\n",
    "    err = mean_squared_error(y_test_mult[:, i], y_test_mult_pred[:, i])\n",
    "    print(\"turbine #\", i+1, \"- RMSE:\", sqrt(err))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7929a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(y_test_mult_pred.shape[1]):\n",
    "    print(f\"Turbine #{i+1}:\") \n",
    "    plot_means_variances(y_test_mult[:, i], y_test_mult_pred[:, i], y_test_mult_stddevs[:, i], save_path=f\"figures/{model_path}/preds{i+1}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76e40c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(y_test_mult_pred.shape[1]):\n",
    "    print(f\"Turbine #{i+1}:\")\n",
    "    plot_confidence_interval_bar(y_test_mult_pred[:, i], y_test_mult_stddevs[:, i], y_test_mult[:, i], bins=20, save_path=f\"figures/{model_path}/cis{i+1}.png\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "50961642",
   "metadata": {},
   "source": [
    "While we acknowledge that these multivariate models do not perform as accurately as its univariate counterparts, the exploration of this multivariate study leads to believe that, with larger data sets and greater computational resources, larger probabilistic networks could be trained to perform a probabilistic deep learning multivariate normal behaviour modelling of the power output of entire wind farms. This is something that, to the author's knowledge, has never been achieved before. In particular the limited data set of 73k data points for a very large number of tunable parameters, for example more than 150k in the network modelling covariance, does not allow for sufficiently reduced predictive variance, quantified in the amount of variation of the target function when considering a different dataset, of the network given its large capacity. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e493e526",
   "metadata": {},
   "source": [
    "## Wind Farm Pre-training and Winf Turbine Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fce94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file path for saving the weights\n",
    "checkpoint_path = 'saved_models/pretrain.h5'\n",
    "\n",
    "# Define the initial model architecture\n",
    "def generic_model(X_train_full):\n",
    "    inputs = Input(shape=(X_train_full.shape[1],))\n",
    "    hidden1 = Dense(300, activation=\"relu\")(inputs)\n",
    "    hidden2 = Dense(200, activation=\"relu\")(hidden1)\n",
    "    hidden3 = Dense(100, activation=\"relu\")(hidden2)\n",
    "\n",
    "    params = Dense(2)(hidden3)\n",
    "\n",
    "    dist = tfp.layers.DistributionLambda(normal_softplus)(params)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=dist)\n",
    "    model.compile(Adam(learning_rate=0.001), loss=NLL)\n",
    "\n",
    "    return model\n",
    "\n",
    "# Train the initial model using X_full with the checkpoint callback\n",
    "generic_model = generic_model(X_train_full)\n",
    "generic_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09981b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the callback to save the weights\n",
    "checkpoint_callback = ModelCheckpoint(filepath=checkpoint_path, save_weights_only=True,\n",
    "                                      monitor='val_loss', mode='min', save_best_only=True)\n",
    "\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=30, restore_best_weights=True)\n",
    "\n",
    "# Load weights from the checkpoint if available\n",
    "if os.path.exists(checkpoint_path):\n",
    "    print(\"Checkpoint found. Loading weights.\")\n",
    "    generic_model.load_weights(checkpoint_path)\n",
    "    # Get the number of epochs already run\n",
    "    start_epoch = generic_model.history.epoch[-1]\n",
    "    print(f\"Resuming training from epoch {start_epoch}.\")\n",
    "else:\n",
    "    print(\"No checkpoint found. Training from scratch.\")\n",
    "    start_epoch = 0\n",
    "\n",
    "history = generic_model.fit(X_train_full, y_train_full, epochs=5*HUNDRED, batch_size=64, initial_epoch=start_epoch,\n",
    "                            validation_split=0.1, callbacks=[checkpoint_callback, early_stopping_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6593480f",
   "metadata": {},
   "outputs": [],
   "source": [
    "generic_model.load_weights(checkpoint_path)\n",
    "evaluation = generic_model.evaluate(X_test_full, y_test_full)\n",
    "print(\"Evaluation Loss:\", evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11aee7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = 'saved_models/finetuned.keras'\n",
    "\n",
    "def create_model_finetune(X_train, generic_model, seed):\n",
    "    keras.utils.set_random_seed(seed)\n",
    "\n",
    "    inputs = Input(shape=(X_train.shape[1],))\n",
    "\n",
    "    # Step 1: Load the architecture and weights of the previously trained model\n",
    "    pretrained_model_layers = generic_model.layers[1:]\n",
    "    l = inputs\n",
    "\n",
    "    for layer in pretrained_model_layers:\n",
    "        layer.trainable = True\n",
    "        l = layer(l)\n",
    "\n",
    "    model_mlp_gaussian = Model(inputs=inputs, outputs=l)\n",
    "    model_mlp_gaussian.compile(Adam(learning_rate=1e-4), loss=NLL)\n",
    "\n",
    "    return model_mlp_gaussian\n",
    "\n",
    "model_finetune = create_model_finetune(X_train, generic_model, MODELS_SEED)\n",
    "model_finetune.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a987351c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the callback to save the weights\n",
    "checkpoint_callback = ModelCheckpoint(filepath=checkpoint_path, save_weights_only=True,\n",
    "                                      monitor='val_loss', mode='min', save_best_only=True)\n",
    "\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "history = model_finetune.fit(X_train, y_train, epochs=50, batch_size=32,\n",
    "                            validation_split=0.1, callbacks=[checkpoint_callback, early_stopping_callback])\n",
    "plot_loss_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9740ea7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "generic_model.load_weights(checkpoint_path)\n",
    "evaluation = model_finetune.evaluate(X_test, y_test)\n",
    "print(\"Evaluation Loss:\", evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72aeb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = np.array(model_finetune(X_train).mean()).ravel()\n",
    "y_test_pred = np.array(model_finetune(X_test).mean()).ravel()\n",
    "\n",
    "y_train_stddevs = np.array(model_finetune(X_train).stddev()).ravel()\n",
    "y_test_stddevs = np.array(model_finetune(X_test).stddev()).ravel()\n",
    "\n",
    "name = \"Fine-tuned\"\n",
    "\n",
    "save_preds(name, y_test_pred, y_test_stddevs)\n",
    "\n",
    "evaluate_and_save_metrics(name,\n",
    "    y_train, y_test, y_train_pred, y_test_pred,\n",
    "    y_train_stddevs, y_test_stddevs, 0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100c5ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_means_variances(y_test, y_test_pred, y_test_stddevs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c837cc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confidence_interval_bar(y_test_pred, y_test_stddevs, y_test, bins=20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2d4ae23d",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5452731d",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.read_csv('results.csv')\n",
    "\n",
    "# Print results\n",
    "for idx, row in results.iterrows():\n",
    "    print(\"Model:\", row['Model Name'])\n",
    "    for metric, value in row.items():\n",
    "        if metric != 'Model Name':\n",
    "            print(f\"{metric:50} {value if isinstance(value, str) else round(value, 2)}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289d545a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.read_csv('results.csv')\n",
    "\n",
    "# Get the list of metrics\n",
    "metrics = results.columns[1:]\n",
    "\n",
    "# Print results\n",
    "for idx, row in results.iterrows():\n",
    "    # print(row['Model Name'])\n",
    "    formatted_line = \"\"\n",
    "    for i, metric in enumerate(metrics):\n",
    "        value = row[metric]\n",
    "        # Try converting string values to floats\n",
    "        try:\n",
    "            value = float(value)\n",
    "        except ValueError:\n",
    "            pass  # If conversion fails, keep the original value\n",
    "        # Format the value to two decimal places\n",
    "        formatted_value = \"{:.2f}\".format(value) if isinstance(value, float) else value\n",
    "        if i == len(metrics) - 1:\n",
    "            formatted_value += \"\\%\" \n",
    "        \n",
    "        formatted_line += f\" & {formatted_value}\"\n",
    "        if i == 0:\n",
    "            formatted_line = row['Model Name'] + \" \" + formatted_value\n",
    "    print(formatted_line + \" \\\\\\\\\")\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9c609b6d",
   "metadata": {},
   "source": [
    "### Deep Neural Network with Gaussian Output (Joint Submodels) - Averaged over Seeds"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "62632331",
   "metadata": {},
   "source": [
    "We average our proposed model over 5 different random seed for initializing the vector weight to show that our model is robust to the randomness of weight initialization. We demonstrate this by showing a very low standard deviation over the sample set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40d7fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED_BIAS_CHECK = False\n",
    "if SEED_BIAS_CHECK:\n",
    "    rmses = []\n",
    "    maes = []\n",
    "    percentages = []\n",
    "    NUM_SEED = 5\n",
    "\n",
    "    for seed in range(NUM_SEED):\n",
    "        model_mlp_gaussian_joint = create_model_mlp_gaussian_joint(X_train, seed)\n",
    "        cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=overwrite(f\"saved_models/model_mlp_gaussian_joint_{seed}.keras\"),\n",
    "                                                     save_weights_only=True,\n",
    "                                                     verbose=0)\n",
    "        history = train_model(model_mlp_gaussian_joint,\n",
    "                          X_train, y_train,\n",
    "                          patience=15, epochs=100, batch_size=32,\n",
    "                          cp_callback=cp_callback,,\n",
    "                          seed=MODELS_SEED)\n",
    "\n",
    "        z_value = stats.norm.ppf((1 + CONFIDENCE_INTERVAL) / 2)\n",
    "\n",
    "        y_test_pred = np.array(model_mlp_gaussian_joint(X_test).mean()).ravel()\n",
    "        y_test_stddevs = np.array(model_mlp_gaussian_joint(X_test).stddev()).ravel()\n",
    "\n",
    "        test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "        test_mae = np.sqrt(mean_absolute_error(y_test, y_test_pred))\n",
    "\n",
    "        test_lower_bound = y_test_pred - z_value * y_test_stddevs\n",
    "        test_upper_bound = y_test_pred + z_value * y_test_stddevs\n",
    "\n",
    "        test_within_interval = np.sum(np.logical_and(y_test.ravel() >= test_lower_bound, y_test.ravel() <= test_upper_bound))\n",
    "        test_percentage_within_interval = (test_within_interval / len(y_test.ravel())) * 100\n",
    "\n",
    "        rmses.append(test_rmse)\n",
    "        maes.append(test_mae)\n",
    "        percentages.append(test_percentage_within_interval)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1093c70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SEED_BIAS_CHECK == True:\n",
    "    print(f\"Mean of RMSEs over {NUM_SEED} seeds: {np.array(rmses).mean()}\")\n",
    "    print(f\"Standard deviation of RMSEs over {NUM_SEED} seeds: {np.array(rmses).std()}\")\n",
    "\n",
    "    print(f\"Mean of MAEs over {NUM_SEED} seeds: {np.array(rmses).mean()}\")\n",
    "    print(f\"Standard deviation of MAEs over {NUM_SEED} seeds: {np.array(rmses).std()}\")\n",
    "\n",
    "    print(f\"Mean of percentages in {CONFIDENCE_INTERVAL} C.I. over {NUM_SEED} seeds: {np.array(rmses).mean()}\")\n",
    "    print(f\"Standard deviation of percentages in {CONFIDENCE_INTERVAL} C.I. over {NUM_SEED} seeds: {np.array(rmses).std()}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "85b1be5d",
   "metadata": {},
   "source": [
    "As predicted, the RMSE is inline with what we had for a single seed, moreover, the standard deviation is very low. This shows how our model is robust to the randomness of the weight initialization."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "37eb84d6",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6ceda1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2c1bd7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
