{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "kq0L8j9VkUf_"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import tensorflow.keras as keras\n",
    "import scipy.stats as stats\n",
    "import math\n",
    "import os\n",
    "\n",
    "from sklearn.gaussian_process.kernels import DotProduct, WhiteKernel, RBF, Matern, RationalQuadratic, ExpSineSquared\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, PowerTransformer, QuantileTransformer\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "from math import sqrt\n",
    "\n",
    "from tensorflow.keras.layers import TimeDistributed, Attention, Input, Conv1D, MaxPooling1D, LSTM, Dense, Flatten, BatchNormalization, Concatenate\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "from keras.layers import Input, Dense, Dropout\n",
    "from keras.models import Model, Sequential\n",
    "from keras.utils import plot_model\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow_probability import distributions as tfd\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import norm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from matplotlib.ticker import FuncFormatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "Fv-KvXtgkVDF"
   },
   "outputs": [],
   "source": [
    "def plot_means_variances(y_true, y_means, y_stddevs):\n",
    "    plt.rc('font', size=14)\n",
    "    min_vals = np.min([np.min(y_true), np.min(y_means)])\n",
    "    max_vals = np.max([np.max(y_true), np.max(y_means)])\n",
    "\n",
    "    plt.figure(figsize=(16, 6))\n",
    "\n",
    "    # Plot predicted vs true\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.scatter(y_true, y_means, alpha = .7, color=\"0.3\", linewidth = 0, s = 2)\n",
    "    plt.plot([min_vals, max_vals], [min_vals, max_vals], 'k--', color='red')  # Add diagonal line\n",
    "    plt.title('Fig (a): Predicted vs True Values')\n",
    "    plt.xlabel('True Values')\n",
    "    plt.ylabel('Predicted Values')\n",
    "    \n",
    "    def plot_binned_residuals(y_true, residuals, num_bins=20):\n",
    "        bins = np.linspace(min(y_true), max(y_true), num_bins + 1)\n",
    "\n",
    "        bin_means = [0]*num_bins\n",
    "        bin_stddevs = [0]*num_bins\n",
    "\n",
    "        for i in range(num_bins):\n",
    "            mask = (y_true >= bins[i]) & (y_true < bins[i + 1])\n",
    "            if np.any(mask):\n",
    "                bin_means[i] = np.mean(y_true[mask])\n",
    "                bin_stddevs[i] = np.sqrt(mean_squared_error(y_means[mask], y_true[mask]))\n",
    "        return bin_means, bin_stddevs\n",
    "\n",
    "    bin_means, bin_stddevs = plot_binned_residuals(y_true, y_means, num_bins=20)\n",
    "    \n",
    "    # Plot residuals vs true\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.scatter(y_true, y_stddevs, alpha = .7, color=\"0.3\", linewidth = 0, s = 2, label='Predicted Standard Deviation', zorder=1)\n",
    "    plt.scatter(bin_means, bin_stddevs, alpha=1, s=50, color='red', label='True Binned Root Mean Squared Error', zorder=2)\n",
    "    plt.title('Fig (b): Predicted Standard Deviation vs True RMSE')\n",
    "    plt.xlabel('True Values')\n",
    "    plt.ylabel('Predicted Standard Deviation')\n",
    "    plt.legend()\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "def evaluate_and_print_metrics(results, model_name, y_train, y_test, y_train_pred, y_test_pred, y_train_stddevs, y_test_stddevs, ci):\n",
    "    z_value = stats.norm.ppf((1 + ci) / 2)\n",
    "    \n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "\n",
    "    train_mae = mean_absolute_error(y_train, y_train_pred)    # in %\n",
    "    test_mae = mean_absolute_error(y_test, y_test_pred)       # in %\n",
    "\n",
    "    train_lower_bound = y_train_pred - z_value * y_train_stddevs\n",
    "    train_upper_bound = y_train_pred + z_value * y_train_stddevs\n",
    "\n",
    "    test_lower_bound = y_test_pred - z_value * y_test_stddevs\n",
    "    test_upper_bound = y_test_pred + z_value * y_test_stddevs\n",
    "\n",
    "    train_within_interval = np.sum(np.logical_and(y_train.ravel() >= train_lower_bound, y_train.ravel() <= train_upper_bound))\n",
    "    test_within_interval = np.sum(np.logical_and(y_test.ravel() >= test_lower_bound, y_test.ravel() <= test_upper_bound))\n",
    "\n",
    "    train_percentage_within_interval = (train_within_interval / len(y_train.ravel())) * 100\n",
    "    test_percentage_within_interval = (test_within_interval / len(y_test.ravel())) * 100\n",
    "\n",
    "   \n",
    "    results[model_name] = {\n",
    "        \"Test Root Mean Squared Error (RMSE): \": test_rmse,\n",
    "        \"Test Mean Absolute Error (MAE): \": test_mae,\n",
    "        f\"Percentage of Test Data Points within {ci*100:.2f}% CI: \": test_percentage_within_interval\n",
    "    }\n",
    "\n",
    "    print(f\"Train RMSE: {train_rmse:.3f}\")\n",
    "    print(f\"Test RMSE: {test_rmse:.3f}\")\n",
    "    print(f\"Train MAE: {train_mae:.3f}\")\n",
    "    print(f\"Test MAE: {test_mae:.3f}\")\n",
    "    print(f\"Percentage of Train Data Points within {ci*100:.2f}% CI: {train_percentage_within_interval:.2f}%\")\n",
    "    print(f\"Percentage of Test Data Points within {ci*100:.2f}% CI: {test_percentage_within_interval:.2f}%\")\n",
    "    \n",
    "def plot_confidence_interval_scatter(y_test_pred, y_test_std, y_test, bins=20):\n",
    "    plt.rc('font', size=14)\n",
    "    \n",
    "    # Compute the t-values of the confidence intervals based on Z-scores\n",
    "    t_values = np.array([stats.norm.ppf(i/bins + (1-i/bins)/2) for i in range(1, bins+1)])\n",
    "\n",
    "    percentages_within_interval = []\n",
    "    for t_value in t_values:\n",
    "        lower_bounds = y_test_pred.ravel() - t_value * y_test_std\n",
    "        upper_bounds = y_test_pred.ravel() + t_value * y_test_std\n",
    "\n",
    "        # Count number of data points within the confidence interval\n",
    "        is_within_interval = np.logical_and(y_test >= lower_bounds, y_test <= upper_bounds)\n",
    "        num_within_interval = np.sum(is_within_interval)\n",
    "\n",
    "        # Calculate the percentage of data points within the confidence interval\n",
    "        percentage_within_interval = (num_within_interval / len(y_test)) * 100\n",
    "        percentages_within_interval.append(percentage_within_interval)\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.scatter(np.arange(1, bins+1)*100/bins, percentages_within_interval, color='blue', label='Percentage of Residuals within Interval')\n",
    "    \n",
    "    # Plot the expected diagonal line (red line)\n",
    "    plt.plot([0, 100], [0, 100], color='red', linestyle='--', label='Expected')\n",
    "\n",
    "    # Add percentage symbols to x-axis ticks\n",
    "    plt.gca().xaxis.set_major_formatter(FuncFormatter(lambda x, _: f'{int(x)}%'))\n",
    "\n",
    "    plt.xlabel('Confidence Intervals')\n",
    "    plt.ylabel('Percentage within Interval')\n",
    "    plt.title('Scatter Plot of Percentage of Residuals within the Confidence Intervals')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "def load_dataset_train_test_split(df, features, output_feature):\n",
    "    X = df[features]\n",
    "    y = df[output_feature]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=42)\n",
    "\n",
    "    # Scale input data to facilitate training\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    return X_train_scaled, X_test_scaled, np.array(y_train), np.array(y_test), scaler\n",
    "\n",
    "def plot_loss_history(history):\n",
    "    plt.plot(history.history['loss'][1:], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'][1:], label='Validation Loss', color='red')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "def compute_predictions(model, X_train, X_test, num_samples=100):\n",
    "    y_train_pred = []\n",
    "    y_test_pred = []\n",
    "    for _ in range(num_samples):\n",
    "        y_train_pred.append(model.predict(X_train))\n",
    "        y_test_pred.append(model.predict(X_test))\n",
    "        \n",
    "    y_train_pred = np.concatenate(y_train_pred, axis=1)\n",
    "    y_test_pred = np.concatenate(y_test_pred, axis=1)\n",
    "\n",
    "    y_train_pred_mean = np.mean(y_train_pred, axis=1)\n",
    "    y_train_pred_stddevs = np.std(y_train_pred, axis=1)\n",
    "    \n",
    "    y_test_pred_mean = np.mean(y_test_pred, axis=1)\n",
    "    y_test_pred_stddevs = np.std(y_test_pred, axis=1)\n",
    "    \n",
    "    return y_train_pred_mean, y_train_pred_stddevs, y_test_pred_mean, y_test_pred_stddevs\n",
    "\n",
    "def NLL(y, distr): \n",
    "    return -distr.log_prob(y) \n",
    "\n",
    "# We add 0.001 to the standard deviation to ensure it does not converge to 0 and destabilizes training because the gradient\n",
    "# of maximum likelihood estimation requires the inversion of the variance. We also activate the parameters using a softplus\n",
    "# activation function to enfore a positive standard deviation estimate.\n",
    "def normal_softplus(params): \n",
    "    return tfd.Normal(loc=params[:, 0:1], scale=1e-3 + tf.math.softplus(0.05 * params[:, 1:2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "QRX2mdorkaEq"
   },
   "outputs": [],
   "source": [
    "# In order to ensure that each model has repeatable results,we fix the seed both for the\n",
    "# data splitting part and for the initilialization of the networks' weights. Theoretially\n",
    "# speaking, we should average over different seeds to ensure the robustness of our results.\n",
    "# However, in practice, due to the size of the data set this is unfeasibile and we only do\n",
    "# this for the best performing model to show that the variability of results based on seed\n",
    "# is almost none.\n",
    "\n",
    "keras.utils.set_random_seed(812)\n",
    "MODELS_SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Dlvvb7ogkaHg",
    "outputId": "dc9d5fea-16e6-4e87-e90a-95d7f08883d5"
   },
   "outputs": [],
   "source": [
    "file_path = 'Cleaned_data.pkl'\n",
    "df_full = pd.read_pickle(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data points before removing NaNs:  1018494\n",
      "Total data points after removing NaNs:  1009707\n",
      "Pre-train Training Set Size:  807764\n",
      "Pre-train Testing Set Size:  201943\n",
      "Finetune Training Set Size:  151504\n",
      "Finetune Testing Set Size:  37877\n"
     ]
    }
   ],
   "source": [
    "# Datetime column\n",
    "DATETIME_COL = 'Date.time'\n",
    "\n",
    "# Features considered\n",
    "features = [\n",
    "'Wind.speed.me',\n",
    "'Wind.speed.sd',\n",
    "'Wind.speed.min',\n",
    "'Wind.speed.max',\n",
    "'Front.bearing.temp.me',\n",
    "'Front.bearing.temp.sd',\n",
    "'Front.bearing.temp.min',\n",
    "'Front.bearing.temp.max',\n",
    "'Rear.bearing.temp.me',\n",
    "'Rear.bearing.temp.sd',\n",
    "'Rear.bearing.temp.min',\n",
    "'Rear.bearing.temp.max',\n",
    "'Rotor.bearing.temp.me',\n",
    "'Stator1.temp.me',\n",
    "'Nacelle.ambient.temp.me',\n",
    "'Nacelle.temp.me',\n",
    "'Transformer.temp.me',\n",
    "'Gear.oil.temp.me',\n",
    "'Gear.oil.inlet.temp.me',\n",
    "'Top.box.temp.me',\n",
    "'Hub.temp.me',\n",
    "'Conv.Amb.temp.me',\n",
    "'Rotor.bearing.temp.me',\n",
    "'Transformer.cell.temp.me',\n",
    "'Motor.axis1.temp.me',\n",
    "'Motor.axis2.temp.me',\n",
    "'CPU.temp.me',\n",
    "'Blade.ang.pitch.pos.A.me',\n",
    "'Blade.ang.pitch.pos.B.me',\n",
    "'Blade.ang.pitch.pos.C.me',\n",
    "'Gear.oil.inlet.press.me',\n",
    "'Gear.oil.pump.press.me',\n",
    "'Drive.train.acceleration.me',\n",
    "'Tower.Acceleration.x',\n",
    "'Tower.Acceleration.y'\n",
    "]\n",
    "\n",
    "output_feature = 'Power.me'\n",
    "TURBINE_ID = 5\n",
    "\n",
    "df = df_full\n",
    "print(f\"Total data points before removing NaNs: \", len(df))\n",
    "df = df.dropna(subset=features + [output_feature] + [DATETIME_COL])\n",
    "print(f\"Total data points after removing NaNs: \", len(df))\n",
    "df = df.reset_index(drop=False)\n",
    "\n",
    "def train_test_split_by_turbine(group, test_size=0.2):\n",
    "    train_set, test_set = train_test_split(group, test_size=test_size, random_state=42)\n",
    "    return train_set, test_set\n",
    "\n",
    "splits = df.groupby('turbine').apply(train_test_split_by_turbine)\n",
    "\n",
    "df_pretrain_train = pd.concat([split[0] for split in splits.tolist()])\n",
    "df_pretrain_test = pd.concat([split[1] for split in splits.tolist()])\n",
    "print(\"Pre-train Training Set Size: \", df_pretrain_train.shape[0])\n",
    "print(\"Pre-train Testing Set Size: \", df_pretrain_test.shape[0])\n",
    "\n",
    "TURBINE_ID = 5\n",
    "df_finetune_train = splits[TURBINE_ID][0]\n",
    "df_finetune_test = splits[TURBINE_ID][1]\n",
    "print(\"Finetune Training Set Size: \", df_finetune_train.shape[0])\n",
    "print(\"Finetune Testing Set Size: \", df_finetune_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_design_matrix(df_train, df_test, features, output_feature):\n",
    "    X_train, y_train = df_train[features].to_numpy(), df_train[output_feature].to_numpy()\n",
    "    X_test, y_test = df_test[features].to_numpy(), df_test[output_feature].to_numpy()\n",
    "\n",
    "    # Scale input data to facilitate training\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    return X_train_scaled, X_test_scaled, y_train, y_test, scaler\n",
    "    \n",
    "X_train_full, X_test_full,\\\n",
    "    y_train_full, y_test_full,\\\n",
    "    scaler_full = create_design_matrix(df_pretrain_train, df_pretrain_test, features, output_feature)\n",
    "\n",
    "X_train_single_turbine, X_test_single_turbine, \\\n",
    "    y_train_single_turbine, y_test_single_turbine, \\\n",
    "    scaler_single_turbine = create_design_matrix(df_finetune_train, df_finetune_test, features, output_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R0KU-4RekVMJ",
    "outputId": "f2c22fb4-9b63-4bd5-dbd0-fdd08751bd9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_9 (InputLayer)        [(None, 35)]              0         \n",
      "                                                                 \n",
      " dense_21 (Dense)            (None, 300)               10800     \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 200)               60200     \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 100)               20100     \n",
      "                                                                 \n",
      " dense_24 (Dense)            (None, 2)                 202       \n",
      "                                                                 \n",
      " distribution_lambda_4 (Dis  ((None, 1),               0         \n",
      " tributionLambda)             (None, 1))                         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 91302 (356.65 KB)\n",
      "Trainable params: 91302 (356.65 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the file path for saving the weights\n",
    "checkpoint_path = 'saved_models/pretrain.h5'\n",
    "\n",
    "# Define the initial model architecture\n",
    "def generic_model(X_train_full):\n",
    "    inputs = Input(shape=(X_train_full.shape[1],))\n",
    "    hidden1 = Dense(300, activation=\"relu\")(inputs)\n",
    "    hidden2 = Dense(200, activation=\"relu\")(hidden1)\n",
    "    hidden3 = Dense(100, activation=\"relu\")(hidden2)\n",
    "\n",
    "    params = Dense(2)(hidden3)\n",
    "\n",
    "    dist = tfp.layers.DistributionLambda(normal_softplus)(params)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=dist)\n",
    "    model.compile(Adam(learning_rate=0.001), loss=NLL)\n",
    "\n",
    "    return model\n",
    "\n",
    "# Train the initial model using X_full with the checkpoint callback\n",
    "generic_model = generic_model(X_train_full)\n",
    "generic_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IWkFe9AHYFIw",
    "outputId": "a90f5f30-bef8-4ab3-f920-761a7aae2fcd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "11360/11360 [==============================] - 54s 5ms/step - loss: 882.2598 - val_loss: 4.7424\n",
      "Epoch 2/500\n",
      "11360/11360 [==============================] - 45s 4ms/step - loss: 4.6531 - val_loss: 4.6093\n",
      "Epoch 3/500\n",
      "11360/11360 [==============================] - 42s 4ms/step - loss: 4.4392 - val_loss: 4.4240\n",
      "Epoch 4/500\n",
      "11360/11360 [==============================] - 42s 4ms/step - loss: 4.3106 - val_loss: 4.3377\n",
      "Epoch 5/500\n",
      "11360/11360 [==============================] - 43s 4ms/step - loss: 4.2118 - val_loss: 4.1565\n",
      "Epoch 6/500\n",
      "11360/11360 [==============================] - 47s 4ms/step - loss: 4.1414 - val_loss: 4.0575\n",
      "Epoch 7/500\n",
      "11360/11360 [==============================] - 35s 3ms/step - loss: 4.0890 - val_loss: 4.0645\n",
      "Epoch 8/500\n",
      "11360/11360 [==============================] - 39s 3ms/step - loss: 4.0467 - val_loss: 4.0506\n",
      "Epoch 9/500\n",
      "11360/11360 [==============================] - 48s 4ms/step - loss: 4.0133 - val_loss: 4.0926\n",
      "Epoch 10/500\n",
      "11360/11360 [==============================] - 46s 4ms/step - loss: 3.9896 - val_loss: 3.9904\n",
      "Epoch 11/500\n",
      "11360/11360 [==============================] - 47s 4ms/step - loss: 3.9670 - val_loss: 4.0001\n",
      "Epoch 12/500\n",
      "11360/11360 [==============================] - 48s 4ms/step - loss: 3.9497 - val_loss: 3.9911\n",
      "Epoch 13/500\n",
      "11360/11360 [==============================] - 43s 4ms/step - loss: 3.9354 - val_loss: 4.1038\n",
      "Epoch 14/500\n",
      "11360/11360 [==============================] - 45s 4ms/step - loss: 3.9185 - val_loss: 3.9797\n",
      "Epoch 15/500\n",
      "11360/11360 [==============================] - 42s 4ms/step - loss: 3.9050 - val_loss: 3.9128\n",
      "Epoch 16/500\n",
      "11360/11360 [==============================] - 40s 4ms/step - loss: 3.8954 - val_loss: 3.8959\n",
      "Epoch 17/500\n",
      "11360/11360 [==============================] - 47s 4ms/step - loss: 3.8842 - val_loss: 3.8929\n",
      "Epoch 18/500\n",
      "11360/11360 [==============================] - 47s 4ms/step - loss: 3.8750 - val_loss: 3.9023\n",
      "Epoch 19/500\n",
      "11360/11360 [==============================] - 46s 4ms/step - loss: 3.8673 - val_loss: 3.9106\n",
      "Epoch 20/500\n",
      "11360/11360 [==============================] - 48s 4ms/step - loss: 3.8604 - val_loss: 3.9477\n",
      "Epoch 21/500\n",
      "11360/11360 [==============================] - 42s 4ms/step - loss: 3.8516 - val_loss: 3.8901\n",
      "Epoch 22/500\n",
      "11360/11360 [==============================] - 39s 3ms/step - loss: 3.8477 - val_loss: 3.8828\n",
      "Epoch 23/500\n",
      "11360/11360 [==============================] - 53s 5ms/step - loss: 3.8401 - val_loss: 3.8516\n",
      "Epoch 24/500\n",
      "11360/11360 [==============================] - 48s 4ms/step - loss: 3.8384 - val_loss: 3.9129\n",
      "Epoch 25/500\n",
      "11360/11360 [==============================] - 51s 4ms/step - loss: 3.8282 - val_loss: 3.8915\n",
      "Epoch 26/500\n",
      "11360/11360 [==============================] - 50s 4ms/step - loss: 3.8261 - val_loss: 3.8435\n",
      "Epoch 27/500\n",
      "11360/11360 [==============================] - 44s 4ms/step - loss: 3.8181 - val_loss: 3.8505\n",
      "Epoch 28/500\n",
      "11360/11360 [==============================] - 47s 4ms/step - loss: 3.8273 - val_loss: 3.8863\n",
      "Epoch 29/500\n",
      "11360/11360 [==============================] - 43s 4ms/step - loss: 3.8160 - val_loss: 3.8317\n",
      "Epoch 30/500\n",
      "11360/11360 [==============================] - 42s 4ms/step - loss: 3.8067 - val_loss: 3.8450\n",
      "Epoch 31/500\n",
      "11360/11360 [==============================] - 42s 4ms/step - loss: 3.8010 - val_loss: 3.8224\n",
      "Epoch 32/500\n",
      "11360/11360 [==============================] - 42s 4ms/step - loss: 3.7982 - val_loss: 3.8740\n",
      "Epoch 33/500\n",
      "11360/11360 [==============================] - 41s 4ms/step - loss: 3.7936 - val_loss: 3.8295\n",
      "Epoch 34/500\n",
      "11360/11360 [==============================] - 42s 4ms/step - loss: 3.7894 - val_loss: 3.8137\n",
      "Epoch 35/500\n",
      "11360/11360 [==============================] - 43s 4ms/step - loss: 3.7877 - val_loss: 3.8199\n",
      "Epoch 36/500\n",
      "11360/11360 [==============================] - 87s 8ms/step - loss: 3.7827 - val_loss: 3.8842\n",
      "Epoch 37/500\n",
      "11360/11360 [==============================] - 43s 4ms/step - loss: 3.7812 - val_loss: 3.8729\n",
      "Epoch 38/500\n",
      "11360/11360 [==============================] - 43s 4ms/step - loss: 3.7808 - val_loss: 3.8522\n",
      "Epoch 39/500\n",
      "11360/11360 [==============================] - 50s 4ms/step - loss: 3.7744 - val_loss: 3.8299\n",
      "Epoch 40/500\n",
      "11360/11360 [==============================] - 51s 4ms/step - loss: 3.7703 - val_loss: 3.8303\n",
      "Epoch 41/500\n",
      "11360/11360 [==============================] - 50s 4ms/step - loss: 3.7682 - val_loss: 3.7912\n",
      "Epoch 42/500\n",
      "11360/11360 [==============================] - 51s 5ms/step - loss: 3.7649 - val_loss: 3.8001\n",
      "Epoch 43/500\n",
      "11360/11360 [==============================] - 52s 5ms/step - loss: 3.7626 - val_loss: 3.8100\n",
      "Epoch 44/500\n",
      "11360/11360 [==============================] - 66s 6ms/step - loss: 3.7600 - val_loss: 3.8071\n",
      "Epoch 45/500\n",
      "11360/11360 [==============================] - 158s 14ms/step - loss: 3.7558 - val_loss: 3.8433\n",
      "Epoch 46/500\n",
      "11360/11360 [==============================] - 168s 15ms/step - loss: 3.7554 - val_loss: 3.7905\n",
      "Epoch 47/500\n",
      "11360/11360 [==============================] - 162s 14ms/step - loss: 3.7527 - val_loss: 3.8559\n",
      "Epoch 48/500\n",
      "11360/11360 [==============================] - 163s 14ms/step - loss: 3.7515 - val_loss: 3.8213\n",
      "Epoch 49/500\n",
      "11360/11360 [==============================] - 161s 14ms/step - loss: 3.7490 - val_loss: 3.7705\n",
      "Epoch 50/500\n",
      " 2928/11360 [======>.......................] - ETA: 1:54 - loss: 3.7475"
     ]
    }
   ],
   "source": [
    "# Define the callback to save the weights\n",
    "checkpoint_callback = ModelCheckpoint(filepath=checkpoint_path, save_weights_only=True,\n",
    "                                      monitor='val_loss', mode='min', save_best_only=True)\n",
    "\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=30, restore_best_weights=True)\n",
    "\n",
    "# Load weights from the checkpoint if available\n",
    "if os.path.exists(checkpoint_path):\n",
    "    print(\"Checkpoint found. Loading weights.\")\n",
    "    generic_model.load_weights(checkpoint_path)\n",
    "    # Get the number of epochs already run\n",
    "    start_epoch = generic_model.history.epoch[-1]\n",
    "    print(f\"Resuming training from epoch {start_epoch}.\")\n",
    "else:\n",
    "    print(\"No checkpoint found. Training from scratch.\")\n",
    "    start_epoch = 0\n",
    "\n",
    "history = generic_model.fit(X_train_full, y_train_full, epochs=500, batch_size=64, initial_epoch=start_epoch,\n",
    "                            validation_split=0.1, callbacks=[checkpoint_callback, early_stopping_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XBFsTIV1Sesa"
   },
   "outputs": [],
   "source": [
    "generic_model.load_weights(checkpoint_path)\n",
    "evaluation = generic_model.evaluate(X_test_full, y_test_full)\n",
    "print(\"Evaluation Loss:\", evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5_jmUfLfSZ9L"
   },
   "outputs": [],
   "source": [
    "def create_model_finetune(X_train, generic_model, seed):\n",
    "    keras.utils.set_random_seed(seed)\n",
    "\n",
    "    inputs = Input(shape=(X_train.shape[1],))\n",
    "\n",
    "    # Step 1: Load the architecture and weights of the previously trained model\n",
    "    pretrained_model_layers = generic_model.layers[1:]\n",
    "    l = inputs\n",
    "\n",
    "    for layer in pretrained_model_layers:\n",
    "        layer.trainable = True\n",
    "        l = layer(l)\n",
    "\n",
    "    model_mlp_gaussian = Model(inputs=inputs, outputs=l)\n",
    "    model_mlp_gaussian.compile(Adam(learning_rate=1e-4), loss=NLL)\n",
    "\n",
    "    return model_mlp_gaussian\n",
    "\n",
    "model_finetune = create_model_finetune(X_train_single_turbine, generic_model, MODELS_SEED)\n",
    "model_finetune.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qtnppfnFS-WK",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define the callback to save the weights\n",
    "checkpoint_callback = ModelCheckpoint(filepath='saved_models/finetuned.keras', save_weights_only=True,\n",
    "                                      monitor='val_loss', mode='min', save_best_only=True)\n",
    "\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "history = model_finetune.fit(X_train_single_turbine, y_train_single_turbine, epochs=50, batch_size=32,\n",
    "                            validation_split=0.1, callbacks=[checkpoint_callback, early_stopping_callback])\n",
    "plot_loss_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generic_model.load_weights(checkpoint_path)\n",
    "evaluation = model_finetune.evaluate(X_test_single_turbine, y_test_single_turbine)\n",
    "print(\"Evaluation Loss:\", evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IqZGOme7S-bb"
   },
   "outputs": [],
   "source": [
    "y_train_pred = np.array(model_finetune(X_train_single_turbine).mean()).ravel()\n",
    "y_test_pred = np.array(model_finetune(X_test_single_turbine).mean()).ravel()\n",
    "\n",
    "y_train_stddevs = np.array(model_finetune(X_train_single_turbine).stddev()).ravel()\n",
    "y_test_stddevs = np.array(model_finetune(X_test_single_turbine).stddev()).ravel()\n",
    "\n",
    "evaluate_and_print_metrics({}, f\"Fine Tuned\",\n",
    "y_train_single_turbine, y_test_single_turbine, y_train_pred, y_test_pred,\n",
    "y_train_stddevs, y_test_stddevs, 0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_means_variances(y_test_single_turbine, y_test_pred, y_test_stddevs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confidence_interval_scatter(y_test_pred, y_test_stddevs, y_test_single_turbine, bins=20)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
